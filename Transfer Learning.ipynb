{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) is an open source library for numerical computation, specializing in machine learning applications.\n",
    "This part aims at using transfer learning technique to classify plankton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you will build\n",
    "\n",
    "In this codelab, you will learn how to run TensorFlow on a single machine, and will train a simple classifier to classify images of plankton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/60.image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jelly](./assets/results_jelly.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using transfer learning, which means we are starting with a model that has been already trained on another problem. We will then retrain it on a similar problem. Deep learning from scratch can take days, but transfer learning can be done in short order.\n",
    "\n",
    "We are going to use a model trained on the [ImageNet](http://image-net.org/) Large Visual Recognition Challenge [dataset](http://www.image-net.org/challenges/LSVRC/2012/). These models can differentiate between 1,000 different classes, like Dalmatian or dishwasher. You will have a choice of model architectures, so you can determine the right tradeoff between speed, size and accuracy for your problem.\n",
    "\n",
    "We will use this same model, but retrain it to tell apart a small number of classes based on our own examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you'll Learn\n",
    "\n",
    "- How to use Python and TensorFlow to train an image classifier\n",
    "- How to classify images with your trained classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you need\n",
    "\n",
    "- A basic Python knowledge\n",
    "- A basic understanding of Linux commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plankton\n",
    "\n",
    "Plankton are critically important to our ecosystem, accounting for more than half the primary productivity on earth and nearly half the total carbon fixed in the global carbon cycle. They form the foundation of aquatic food webs including those of large, important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.\n",
    "\n",
    "![plankton](https://storage.googleapis.com/kaggle-competitions/kaggle/3978/media/Plankton-Diagram3.png)\n",
    "\n",
    "Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of an underwater imagery sensor. This towed, underwater camera system captures microscopic, high-resolution images over large study areas. The images can then be analyzed to assess species populations and distributions.\n",
    "\n",
    "Manual analysis of the imagery is infeasible – it would take a year or more to manually analyze the imagery volume captured in a single day. Automated image classification using machine learning tools is an alternative to the manual approach. Analytics will allow analysis at speeds and scales previously thought impossible. The automated system will have broad applications for assessment of ocean and ecosystem health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and installation \n",
    "\n",
    "### Install TensorFlow\n",
    "\n",
    "Before we can begin the tutorial you need to [install TensorFlow](https://www.tensorflow.org/versions/r1.7/install/) version 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.7.*\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/90/f73789d9f6ecbd629ef646e234af9d15a6fadf81a928f9ae4332ba85fd76/tensorflow-1.7.1-cp36-cp36m-macosx_10_11_x86_64.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.7.*)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==1.7.*)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/45/0ae182eb92a516a2485e803236eaa90b99d17fea0b3e32d891ce5d33893a/grpcio-1.19.0-cp36-cp36m-macosx_10_9_x86_64.whl (1.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 1.2MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow==1.7.*)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.7.*)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 652kB/s a 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.8.0,>=1.7.0 (from tensorflow==1.7.*)\n",
      "  Using cached https://files.pythonhosted.org/packages/0b/ec/65d4e8410038ca2a78c09034094403d231228d0ddcae7d470b223456e55d/tensorboard-1.7.0-py3-none-any.whl\n",
      "Collecting protobuf>=3.4.0 (from tensorflow==1.7.*)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1e/c481d59760cded074d89ff51c99381708111c550ff698934cc296d27df2c/protobuf-3.7.1-cp36-cp36m-macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 1.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow==1.7.*)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /anaconda3/envs/hana/lib/python3.6/site-packages (from tensorflow==1.7.*) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /anaconda3/envs/hana/lib/python3.6/site-packages (from tensorflow==1.7.*) (1.16.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /anaconda3/envs/hana/lib/python3.6/site-packages (from tensorflow==1.7.*) (0.32.3)\n",
      "Collecting bleach==1.5.0 (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*)\n",
      "  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.10 (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/79/84f02539cc181cdbf5ff5a41b9f52cae870b6f632767e43ba6ac70132e92/Werkzeug-0.15.2-py2.py3-none-any.whl (328kB)\n",
      "\u001b[K    100% |████████████████████████████████| 337kB 263kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 527kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999 (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /anaconda3/envs/hana/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow==1.7.*) (40.7.3)\n",
      "Building wheels for collected packages: absl-py\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/i060307/Library/Caches/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "Successfully built absl-py\n",
      "Installing collected packages: termcolor, grpcio, astor, absl-py, html5lib, bleach, protobuf, werkzeug, markdown, tensorboard, gast, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 bleach-1.5.0 gast-0.2.2 grpcio-1.19.0 html5lib-0.9999999 markdown-3.1 protobuf-3.7.1 tensorboard-1.7.0 tensorflow-1.7.1 termcolor-1.1.0 werkzeug-0.15.2\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "!pip install --upgrade \"tensorflow==1.7.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the git repository\n",
    "\n",
    "All the code used in this codelab is contained in this git repository. Clone the repository and cd into it. This is where we will be working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cml-plankton-classifier'...\n",
      "remote: Enumerating objects: 20, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 20 (delta 4), reused 20 (delta 4), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (20/20), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aymen-mouelhi/cml-plankton-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/i060307/Desktop/Projects/cml-plankton/cml-plankton-classifier\n"
     ]
    }
   ],
   "source": [
    "%cd cml-plankton-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCONTRIBUTING.md\u001b[m\u001b[m \u001b[31mLICENSE\u001b[m\u001b[m         README.md       \u001b[1m\u001b[36mscripts\u001b[m\u001b[m         \u001b[1m\u001b[36mtf_files\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the training images\n",
    "\n",
    "Before you start any training, you'll need a set of images to teach the model about the new classes you want to recognize. We've created an archive of creative-commons licensed flower photos to use initially. Download the photos (218 MB) by invoking the following two commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  8  791M    8 65.5M    0     0   133k      0  1:41:13  0:08:23  1:32:50     0  0     0   281k      0  0:47:53  0:00:30  0:47:23  219k272k      0  0:49:29  0:00:36  0:48:53  238k 0   264k      0  0:51:07  0:00:45  0:50:22  153k8M    0     0   261k      0  0:51:36  0:00:46  0:50:50  153k   256k      0  0:52:36  0:01:23  0:51:13  242k   248k      0  0:54:20  0:02:07  0:52:13  156k      0  0:54:53  0:02:15  0:52:38  242k0   251k      0  0:53:41  0:03:22  0:50:19  197k252k      0  0:53:22  0:03:32  0:49:50  190k 242k      0  0:55:37  0:04:10  0:51:27  260kM    0     0   231k      0  0:58:14  0:04:38  0:53:36  129kM    0     0   231k      0  0:58:19  0:04:42  0:53:37  204k 8 65.5M    0     0   217k      0  1:01:59  0:05:08  0:56:51     08 65.5M    0     0   214k      0  1:03:00  0:05:13  0:57:47     0 0     0   212k      0  1:03:24  0:05:15  0:58:09     05.5M    0     0   186k      0  1:12:31  0:06:00  1:06:31     00     0   176k      0  1:16:22  0:06:19  1:10:03     00     0   174k      0  1:17:22  0:06:24  1:10:58     0 0     0   143k      0  1:34:06  0:07:47  1:26:19     0^C\n"
     ]
    }
   ],
   "source": [
    "# Download the training images\n",
    "!curl https://cmlplankton.s3.amazonaws.com/images.zip \\\n",
    "    | tar xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a copy of the flower photos. Confirm the contents of your working directory by issuing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: images: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have something like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/folder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Note: You will need to open a new terminal window and copy the instructions below.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (Re)training the network\n",
    "\n",
    "### Configure your MobileNet\n",
    "\n",
    "In this exercise, we will retrain a [MobileNet](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html). MobileNet is a a small efficient convolutional neural network. \"Convolutional\" just means that the same calculations are performed at each location in the image.\n",
    "\n",
    "The MobileNet is configurable in two ways:\n",
    "\n",
    "Input image resolution: 128,160,192, or 224px. Unsurprisingly, feeding in a higher resolution image takes more processing time, but results in better classification accuracy.\n",
    "The relative size of the model as a fraction of the largest MobileNet: 1.0, 0.75, 0.50, or 0.25.\n",
    "We will use 224 0.5 for this codelab.\n",
    "\n",
    "With the recommended settings, it typically takes only a couple of minutes to retrain on a laptop. You will pass the settings inside Linux shell variables. Set those variables in your shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE=\"mobilenet_0.50_${IMAGE_SIZE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about MobileNet performance (optional)\n",
    "\n",
    "The graph below shows the first-choice-accuracies of these configurations (y-axis), vs the number of calculations required (x-axis), and the size of the model (circle area).\n",
    "\n",
    "16 points are shown for MobileNet. For each of the 4 model sizes (circle area in the figure) there is one point for each image resolution setting. The 128px image size models are represented by the lower-left point in each set, while the 224px models are in the upper right.\n",
    "\n",
    "Other notable architectures are also included for reference. \"GoogleNet\" in this figure is \"Inception V1\" in this table. An extended version of this figure is available in slides 84-89 here.\n",
    "\n",
    "![models](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/img/70170cbb89d318b1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start TensorBoard\n",
    "\n",
    "Before starting the training, launch tensorboard in the background. TensorBoard is a monitoring and inspection tool included with tensorflow. You will use it to monitor the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-04 16:18:11.555560: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "TensorBoard 1.7.0 at http://C02X80SCJGH5:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir tf_files/training_summaries &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "This command will fail with the following error if you already have a tensorboard process running:\n",
    "\n",
    "<span style=\"color:red\">ERROR:tensorflow:TensorBoard attempted to bind to port 6006, but it was already in use</span>\n",
    "\n",
    "You can kill all existing TensorBoard instances with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to kill all existing TensorBoard instances\n",
    "# !pkill -f \"tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate the retraining script\n",
    "\n",
    "The retrain script is from the TensorFlow Hub repo, but it is not installed as part of the pip package. So for simplicity I've included it in the codelab repository. You can run the script using the python command. Take a minute to skim its \"help\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: retrain.py [-h] [--image_dir IMAGE_DIR] [--output_graph OUTPUT_GRAPH]\r\n",
      "                  [--intermediate_output_graphs_dir INTERMEDIATE_OUTPUT_GRAPHS_DIR]\r\n",
      "                  [--intermediate_store_frequency INTERMEDIATE_STORE_FREQUENCY]\r\n",
      "                  [--output_labels OUTPUT_LABELS]\r\n",
      "                  [--summaries_dir SUMMARIES_DIR]\r\n",
      "                  [--how_many_training_steps HOW_MANY_TRAINING_STEPS]\r\n",
      "                  [--learning_rate LEARNING_RATE]\r\n",
      "                  [--testing_percentage TESTING_PERCENTAGE]\r\n",
      "                  [--validation_percentage VALIDATION_PERCENTAGE]\r\n",
      "                  [--eval_step_interval EVAL_STEP_INTERVAL]\r\n",
      "                  [--train_batch_size TRAIN_BATCH_SIZE]\r\n",
      "                  [--test_batch_size TEST_BATCH_SIZE]\r\n",
      "                  [--validation_batch_size VALIDATION_BATCH_SIZE]\r\n",
      "                  [--print_misclassified_test_images] [--model_dir MODEL_DIR]\r\n",
      "                  [--bottleneck_dir BOTTLENECK_DIR]\r\n",
      "                  [--final_tensor_name FINAL_TENSOR_NAME] [--flip_left_right]\r\n",
      "                  [--random_crop RANDOM_CROP] [--random_scale RANDOM_SCALE]\r\n",
      "                  [--random_brightness RANDOM_BRIGHTNESS]\r\n",
      "                  [--architecture ARCHITECTURE]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --image_dir IMAGE_DIR\r\n",
      "                        Path to folders of labeled images.\r\n",
      "  --output_graph OUTPUT_GRAPH\r\n",
      "                        Where to save the trained graph.\r\n",
      "  --intermediate_output_graphs_dir INTERMEDIATE_OUTPUT_GRAPHS_DIR\r\n",
      "                        Where to save the intermediate graphs.\r\n",
      "  --intermediate_store_frequency INTERMEDIATE_STORE_FREQUENCY\r\n",
      "                        How many steps to store intermediate graph. If \"0\"\r\n",
      "                        then will not store.\r\n",
      "  --output_labels OUTPUT_LABELS\r\n",
      "                        Where to save the trained graph's labels.\r\n",
      "  --summaries_dir SUMMARIES_DIR\r\n",
      "                        Where to save summary logs for TensorBoard.\r\n",
      "  --how_many_training_steps HOW_MANY_TRAINING_STEPS\r\n",
      "                        How many training steps to run before ending.\r\n",
      "  --learning_rate LEARNING_RATE\r\n",
      "                        How large a learning rate to use when training.\r\n",
      "  --testing_percentage TESTING_PERCENTAGE\r\n",
      "                        What percentage of images to use as a test set.\r\n",
      "  --validation_percentage VALIDATION_PERCENTAGE\r\n",
      "                        What percentage of images to use as a validation set.\r\n",
      "  --eval_step_interval EVAL_STEP_INTERVAL\r\n",
      "                        How often to evaluate the training results.\r\n",
      "  --train_batch_size TRAIN_BATCH_SIZE\r\n",
      "                        How many images to train on at a time.\r\n",
      "  --test_batch_size TEST_BATCH_SIZE\r\n",
      "                        How many images to test on. This test set is only used\r\n",
      "                        once, to evaluate the final accuracy of the model\r\n",
      "                        after training completes. A value of -1 causes the\r\n",
      "                        entire test set to be used, which leads to more stable\r\n",
      "                        results across runs.\r\n",
      "  --validation_batch_size VALIDATION_BATCH_SIZE\r\n",
      "                        How many images to use in an evaluation batch. This\r\n",
      "                        validation set is used much more often than the test\r\n",
      "                        set, and is an early indicator of how accurate the\r\n",
      "                        model is during training. A value of -1 causes the\r\n",
      "                        entire validation set to be used, which leads to more\r\n",
      "                        stable results across training iterations, but may be\r\n",
      "                        slower on large training sets.\r\n",
      "  --print_misclassified_test_images\r\n",
      "                        Whether to print out a list of all misclassified test\r\n",
      "                        images.\r\n",
      "  --model_dir MODEL_DIR\r\n",
      "                        Path to classify_image_graph_def.pb,\r\n",
      "                        imagenet_synset_to_human_label_map.txt, and\r\n",
      "                        imagenet_2012_challenge_label_map_proto.pbtxt.\r\n",
      "  --bottleneck_dir BOTTLENECK_DIR\r\n",
      "                        Path to cache bottleneck layer values as files.\r\n",
      "  --final_tensor_name FINAL_TENSOR_NAME\r\n",
      "                        The name of the output classification layer in the\r\n",
      "                        retrained graph.\r\n",
      "  --flip_left_right     Whether to randomly flip half of the training images\r\n",
      "                        horizontally.\r\n",
      "  --random_crop RANDOM_CROP\r\n",
      "                        A percentage determining how much of a margin to\r\n",
      "                        randomly crop off the training images.\r\n",
      "  --random_scale RANDOM_SCALE\r\n",
      "                        A percentage determining how much to randomly scale up\r\n",
      "                        the size of the training images by.\r\n",
      "  --random_brightness RANDOM_BRIGHTNESS\r\n",
      "                        A percentage determining how much to randomly multiply\r\n",
      "                        the training image input pixels up or down by.\r\n",
      "  --architecture ARCHITECTURE\r\n",
      "                        Which model architecture to use. 'inception_v3' is the\r\n",
      "                        most accurate, but also the slowest. For faster or\r\n",
      "                        smaller models, chose a MobileNet with the form\r\n",
      "                        'mobilenet_<parameter size>_<input_size>[_quantized]'.\r\n",
      "                        For example, 'mobilenet_1.0_224' will pick a model\r\n",
      "                        that is 17 MB in size and takes 224 pixel input\r\n",
      "                        images, while 'mobilenet_0.25_128_quantized' will\r\n",
      "                        choose a much less accurate, but smaller and faster\r\n",
      "                        network that's 920 KB on disk and takes 128x128\r\n",
      "                        images. See\r\n",
      "                        https://research.googleblog.com/2017/06/mobilenets-\r\n",
      "                        open-source-models-for.html for more information on\r\n",
      "                        Mobilenet.\r\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.retrain -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the training\n",
    "\n",
    "As noted in the introduction, ImageNet models are networks with millions of parameters that can differentiate a large number of classes. We're only training the final layer of that network, so training will end in a reasonable amount of time.\n",
    "\n",
    "Start your retraining with one big command (note the --summaries_dir option, sending training progress reports to the directory that tensorboard is monitoring) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/i060307/Desktop/Projects/cml-plankton/cml-plankton-classifier\n"
     ]
    }
   ],
   "source": [
    "%cd cml-plankton-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;49;00m\r\n",
      "\u001b[37m# you may not use this file except in compliance with the License.\u001b[39;49;00m\r\n",
      "\u001b[37m# You may obtain a copy of the License at\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#     http://www.apache.org/licenses/LICENSE-2.0\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Unless required by applicable law or agreed to in writing, software\u001b[39;49;00m\r\n",
      "\u001b[37m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[39;49;00m\r\n",
      "\u001b[37m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[39;49;00m\r\n",
      "\u001b[37m# See the License for the specific language governing permissions and\u001b[39;49;00m\r\n",
      "\u001b[37m# limitations under the License.\u001b[39;49;00m\r\n",
      "\u001b[37m# ==============================================================================\u001b[39;49;00m\r\n",
      "\u001b[33mr\u001b[39;49;00m\u001b[33m\"\"\"Simple transfer learning with Inception v3 or Mobilenet models.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mWith support for TensorBoard.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mThis example shows how to take a Inception v3 or Mobilenet model trained on\u001b[39;49;00m\r\n",
      "\u001b[33mImageNet images, and train a new top layer that can recognize other classes of\u001b[39;49;00m\r\n",
      "\u001b[33mimages.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mThe top layer receives as input a 2048-dimensional vector (1001-dimensional for\u001b[39;49;00m\r\n",
      "\u001b[33mMobilenet) for each image. We train a softmax layer on top of this\u001b[39;49;00m\r\n",
      "\u001b[33mrepresentation. Assuming the softmax layer contains N labels, this corresponds\u001b[39;49;00m\r\n",
      "\u001b[33mto learning N + 2048*N (or 1001*N)  model parameters corresponding to the\u001b[39;49;00m\r\n",
      "\u001b[33mlearned biases and weights.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mHere's an example, which assumes you have a folder containing class-named\u001b[39;49;00m\r\n",
      "\u001b[33msubfolders, each full of images for each label. The example folder downloads\u001b[39;49;00m\r\n",
      "\u001b[33mshould have a structure like this:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m~/downloads/daisy/photo1.jpg\u001b[39;49;00m\r\n",
      "\u001b[33m~/downloads/daisy/photo2.jpg\u001b[39;49;00m\r\n",
      "\u001b[33m...\u001b[39;49;00m\r\n",
      "\u001b[33m~/downloads/rose/anotherphoto77.jpg\u001b[39;49;00m\r\n",
      "\u001b[33m...\u001b[39;49;00m\r\n",
      "\u001b[33m~/downloads/sunflower/somepicture.jpg\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mThe subfolder names are important, since they define what label is applied to\u001b[39;49;00m\r\n",
      "\u001b[33meach image, but the filenames themselves don't matter. Once your images are\u001b[39;49;00m\r\n",
      "\u001b[33mprepared, you can run the training with a command like this:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m```bash\u001b[39;49;00m\r\n",
      "\u001b[33mbazel build tensorflow/examples/image_retraining:retrain && \\\u001b[39;49;00m\r\n",
      "\u001b[33mbazel-bin/tensorflow/examples/image_retraining/retrain \\\u001b[39;49;00m\r\n",
      "\u001b[33m    --image_dir ~/downloads\u001b[39;49;00m\r\n",
      "\u001b[33m```\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mOr, if you have a pip installation of tensorflow, `retrain.py` can be run\u001b[39;49;00m\r\n",
      "\u001b[33mwithout bazel:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m```bash\u001b[39;49;00m\r\n",
      "\u001b[33mpython tensorflow/examples/image_retraining/retrain.py \\\u001b[39;49;00m\r\n",
      "\u001b[33m    --image_dir ~/downloads\u001b[39;49;00m\r\n",
      "\u001b[33m```\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mYou can replace the image_dir argument with any folder containing subfolders of\u001b[39;49;00m\r\n",
      "\u001b[33mimages. The label for each image is taken from the name of the subfolder it's\u001b[39;49;00m\r\n",
      "\u001b[33min.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mThis produces a new model file that can be loaded and run by any TensorFlow\u001b[39;49;00m\r\n",
      "\u001b[33mprogram, for example the label_image sample code.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mBy default this script will use the high accuracy, but comparatively large and\u001b[39;49;00m\r\n",
      "\u001b[33mslow Inception v3 model architecture. It's recommended that you start with this\u001b[39;49;00m\r\n",
      "\u001b[33mto validate that you have gathered good training data, but if you want to deploy\u001b[39;49;00m\r\n",
      "\u001b[33mon resource-limited platforms, you can try the `--architecture` flag with a\u001b[39;49;00m\r\n",
      "\u001b[33mMobilenet model. For example:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m```bash\u001b[39;49;00m\r\n",
      "\u001b[33mpython tensorflow/examples/image_retraining/retrain.py \\\u001b[39;49;00m\r\n",
      "\u001b[33m    --image_dir ~/downloads --architecture mobilenet_1.0_224\u001b[39;49;00m\r\n",
      "\u001b[33m```\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mThere are 32 different Mobilenet models to choose from, with a variety of file\u001b[39;49;00m\r\n",
      "\u001b[33msize and latency options. The first number can be '1.0', '0.75', '0.50', or\u001b[39;49;00m\r\n",
      "\u001b[33m'0.25' to control the size, and the second controls the input image size, either\u001b[39;49;00m\r\n",
      "\u001b[33m'224', '192', '160', or '128', with smaller sizes running faster. See\u001b[39;49;00m\r\n",
      "\u001b[33mhttps://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html\u001b[39;49;00m\r\n",
      "\u001b[33mfor more information on Mobilenet.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mTo use with TensorBoard:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mBy default, this script will log summaries to /tmp/retrain_logs directory\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mVisualize the summaries with this command:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33mtensorboard --logdir /tmp/retrain_logs\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m absolute_import\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m division\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mhashlib\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos.path\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msix.moves\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m urllib\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.framework\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m graph_util\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.framework\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m tensor_shape\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.platform\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gfile\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.util\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m compat\r\n",
      "\r\n",
      "FLAGS = \u001b[36mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# These are all parameters that are tied to the particular model architecture\u001b[39;49;00m\r\n",
      "\u001b[37m# we're using for Inception v3. These include things like tensor names and their\u001b[39;49;00m\r\n",
      "\u001b[37m# sizes. If you want to adapt this script to work with another model, you will\u001b[39;49;00m\r\n",
      "\u001b[37m# need to update these to reflect the values in the network you're using.\u001b[39;49;00m\r\n",
      "MAX_NUM_IMAGES_PER_CLASS = \u001b[34m2\u001b[39;49;00m ** \u001b[34m27\u001b[39;49;00m - \u001b[34m1\u001b[39;49;00m  \u001b[37m# ~134M\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_image_lists\u001b[39;49;00m(image_dir, testing_percentage, validation_percentage):\r\n",
      "  \u001b[33m\"\"\"Builds a list of training images from the file system.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Analyzes the sub folders in the image directory, splits them into stable\u001b[39;49;00m\r\n",
      "\u001b[33m  training, testing, and validation sets, and returns a data structure\u001b[39;49;00m\r\n",
      "\u001b[33m  describing the lists of images for each label and their paths.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    image_dir: String path to a folder containing subfolders of images.\u001b[39;49;00m\r\n",
      "\u001b[33m    testing_percentage: Integer percentage of the images to reserve for tests.\u001b[39;49;00m\r\n",
      "\u001b[33m    validation_percentage: Integer percentage of images reserved for validation.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    A dictionary containing an entry for each label subfolder, with images split\u001b[39;49;00m\r\n",
      "\u001b[33m    into training, testing, and validation sets within each label.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m gfile.Exists(image_dir):\r\n",
      "    tf.logging.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mImage directory \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + image_dir + \u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m not found.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\r\n",
      "  result = collections.OrderedDict()\r\n",
      "  sub_dirs = [\r\n",
      "    os.path.join(image_dir,item)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m gfile.ListDirectory(image_dir)]\r\n",
      "  sub_dirs = \u001b[36msorted\u001b[39;49;00m(item \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m sub_dirs\r\n",
      "                    \u001b[34mif\u001b[39;49;00m gfile.IsDirectory(item))\r\n",
      "  \u001b[34mfor\u001b[39;49;00m sub_dir \u001b[35min\u001b[39;49;00m sub_dirs:\r\n",
      "    extensions = [\u001b[33m'\u001b[39;49;00m\u001b[33mjpg\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mjpeg\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mJPG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mJPEG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    file_list = []\r\n",
      "    dir_name = os.path.basename(sub_dir)\r\n",
      "    \u001b[34mif\u001b[39;49;00m dir_name == image_dir:\r\n",
      "      \u001b[34mcontinue\u001b[39;49;00m\r\n",
      "    tf.logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mLooking for images in \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + dir_name + \u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m extension \u001b[35min\u001b[39;49;00m extensions:\r\n",
      "      file_glob = os.path.join(image_dir, dir_name, \u001b[33m'\u001b[39;49;00m\u001b[33m*.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + extension)\r\n",
      "      file_list.extend(gfile.Glob(file_glob))\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m file_list:\r\n",
      "      tf.logging.warning(\u001b[33m'\u001b[39;49;00m\u001b[33mNo files found\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "      \u001b[34mcontinue\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(file_list) < \u001b[34m20\u001b[39;49;00m:\r\n",
      "      tf.logging.warning(\r\n",
      "          \u001b[33m'\u001b[39;49;00m\u001b[33mWARNING: Folder has less than 20 images, which may cause issues.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(file_list) > MAX_NUM_IMAGES_PER_CLASS:\r\n",
      "      tf.logging.warning(\r\n",
      "          \u001b[33m'\u001b[39;49;00m\u001b[33mWARNING: Folder {} has more than {} images. Some images will \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "          \u001b[33m'\u001b[39;49;00m\u001b[33mnever be selected.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dir_name, MAX_NUM_IMAGES_PER_CLASS))\r\n",
      "    label_name = re.sub(\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m[^a-z0-9]+\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dir_name.lower())\r\n",
      "    training_images = []\r\n",
      "    testing_images = []\r\n",
      "    validation_images = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m file_name \u001b[35min\u001b[39;49;00m file_list:\r\n",
      "      base_name = os.path.basename(file_name)\r\n",
      "      \u001b[37m# We want to ignore anything after '_nohash_' in the file name when\u001b[39;49;00m\r\n",
      "      \u001b[37m# deciding which set to put an image in, the data set creator has a way of\u001b[39;49;00m\r\n",
      "      \u001b[37m# grouping photos that are close variations of each other. For example\u001b[39;49;00m\r\n",
      "      \u001b[37m# this is used in the plant disease data set to group multiple pictures of\u001b[39;49;00m\r\n",
      "      \u001b[37m# the same leaf.\u001b[39;49;00m\r\n",
      "      hash_name = re.sub(\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m_nohash_.*$\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, file_name)\r\n",
      "      \u001b[37m# This looks a bit magical, but we need to decide whether this file should\u001b[39;49;00m\r\n",
      "      \u001b[37m# go into the training, testing, or validation sets, and we want to keep\u001b[39;49;00m\r\n",
      "      \u001b[37m# existing files in the same set even if more files are subsequently\u001b[39;49;00m\r\n",
      "      \u001b[37m# added.\u001b[39;49;00m\r\n",
      "      \u001b[37m# To do that, we need a stable way of deciding based on just the file name\u001b[39;49;00m\r\n",
      "      \u001b[37m# itself, so we do a hash of that and then use that to generate a\u001b[39;49;00m\r\n",
      "      \u001b[37m# probability value that we use to assign it.\u001b[39;49;00m\r\n",
      "      hash_name_hashed = hashlib.sha1(compat.as_bytes(hash_name)).hexdigest()\r\n",
      "      percentage_hash = ((\u001b[36mint\u001b[39;49;00m(hash_name_hashed, \u001b[34m16\u001b[39;49;00m) %\r\n",
      "                          (MAX_NUM_IMAGES_PER_CLASS + \u001b[34m1\u001b[39;49;00m)) *\r\n",
      "                         (\u001b[34m100.0\u001b[39;49;00m / MAX_NUM_IMAGES_PER_CLASS))\r\n",
      "      \u001b[34mif\u001b[39;49;00m percentage_hash < validation_percentage:\r\n",
      "        validation_images.append(base_name)\r\n",
      "      \u001b[34melif\u001b[39;49;00m percentage_hash < (testing_percentage + validation_percentage):\r\n",
      "        testing_images.append(base_name)\r\n",
      "      \u001b[34melse\u001b[39;49;00m:\r\n",
      "        training_images.append(base_name)\r\n",
      "    result[label_name] = {\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mdir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: dir_name,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: training_images,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mtesting\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: testing_images,\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: validation_images,\r\n",
      "    }\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m result\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_image_path\u001b[39;49;00m(image_lists, label_name, index, image_dir, category):\r\n",
      "  \u001b[33m\"\"\"\"Returns a path to an image for a label at the given index.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    image_lists: Dictionary of training images for each label.\u001b[39;49;00m\r\n",
      "\u001b[33m    label_name: Label string we want to get an image for.\u001b[39;49;00m\r\n",
      "\u001b[33m    index: Int offset of the image we want. This will be moduloed by the\u001b[39;49;00m\r\n",
      "\u001b[33m    available number of images for the label, so it can be arbitrarily large.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_dir: Root folder string of the subfolders containing the training\u001b[39;49;00m\r\n",
      "\u001b[33m    images.\u001b[39;49;00m\r\n",
      "\u001b[33m    category: Name string of set to pull images from - training, testing, or\u001b[39;49;00m\r\n",
      "\u001b[33m    validation.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    File system path string to an image that meets the requested parameters.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[34mif\u001b[39;49;00m label_name \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m image_lists:\r\n",
      "    tf.logging.fatal(\u001b[33m'\u001b[39;49;00m\u001b[33mLabel does not exist \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, label_name)\r\n",
      "  label_lists = image_lists[label_name]\r\n",
      "  \u001b[34mif\u001b[39;49;00m category \u001b[35mnot\u001b[39;49;00m \u001b[35min\u001b[39;49;00m label_lists:\r\n",
      "    tf.logging.fatal(\u001b[33m'\u001b[39;49;00m\u001b[33mCategory does not exist \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, category)\r\n",
      "  category_list = label_lists[category]\r\n",
      "  \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m category_list:\r\n",
      "    tf.logging.fatal(\u001b[33m'\u001b[39;49;00m\u001b[33mLabel \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m has no images in the category \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                     label_name, category)\r\n",
      "  mod_index = index % \u001b[36mlen\u001b[39;49;00m(category_list)\r\n",
      "  base_name = category_list[mod_index]\r\n",
      "  sub_dir = label_lists[\u001b[33m'\u001b[39;49;00m\u001b[33mdir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "  full_path = os.path.join(image_dir, sub_dir, base_name)\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m full_path\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_bottleneck_path\u001b[39;49;00m(image_lists, label_name, index, bottleneck_dir,\r\n",
      "                        category, architecture):\r\n",
      "  \u001b[33m\"\"\"\"Returns a path to a bottleneck file for a label at the given index.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    image_lists: Dictionary of training images for each label.\u001b[39;49;00m\r\n",
      "\u001b[33m    label_name: Label string we want to get an image for.\u001b[39;49;00m\r\n",
      "\u001b[33m    index: Integer offset of the image we want. This will be moduloed by the\u001b[39;49;00m\r\n",
      "\u001b[33m    available number of images for the label, so it can be arbitrarily large.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_dir: Folder string holding cached files of bottleneck values.\u001b[39;49;00m\r\n",
      "\u001b[33m    category: Name string of set to pull images from - training, testing, or\u001b[39;49;00m\r\n",
      "\u001b[33m    validation.\u001b[39;49;00m\r\n",
      "\u001b[33m    architecture: The name of the model architecture.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    File system path string to an image that meets the requested parameters.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m get_image_path(image_lists, label_name, index, bottleneck_dir,\r\n",
      "                        category) + \u001b[33m'\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + architecture + \u001b[33m'\u001b[39;49;00m\u001b[33m.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_model_graph\u001b[39;49;00m(model_info):\r\n",
      "  \u001b[33m\"\"\"\"Creates a graph from saved GraphDef file and returns a Graph object.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    model_info: Dictionary containing information about the model architecture.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    Graph holding the trained Inception network, and various tensors we'll be\u001b[39;49;00m\r\n",
      "\u001b[33m    manipulating.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[34mwith\u001b[39;49;00m tf.Graph().as_default() \u001b[34mas\u001b[39;49;00m graph:\r\n",
      "    model_path = os.path.join(FLAGS.model_dir, model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mmodel_file_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \u001b[34mwith\u001b[39;49;00m gfile.FastGFile(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "      graph_def = tf.GraphDef()\r\n",
      "      graph_def.ParseFromString(f.read())\r\n",
      "      bottleneck_tensor, resized_input_tensor = (tf.import_graph_def(\r\n",
      "          graph_def,\r\n",
      "          name=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "          return_elements=[\r\n",
      "              model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mbottleneck_tensor_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "              model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mresized_input_tensor_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "          ]))\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m graph, bottleneck_tensor, resized_input_tensor\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mrun_bottleneck_on_image\u001b[39;49;00m(sess, image_data, image_data_tensor,\r\n",
      "                            decoded_image_tensor, resized_input_tensor,\r\n",
      "                            bottleneck_tensor):\r\n",
      "  \u001b[33m\"\"\"Runs inference on an image to extract the 'bottleneck' summary layer.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    sess: Current active TensorFlow Session.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_data: String of raw JPEG data.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_data_tensor: Input data layer in the graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    decoded_image_tensor: Output of initial image resizing and  preprocessing.\u001b[39;49;00m\r\n",
      "\u001b[33m    resized_input_tensor: The input node of the recognition graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_tensor: Layer before the final softmax.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    Numpy array of bottleneck values.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[37m# First decode the JPEG image, resize it, and rescale the pixel values.\u001b[39;49;00m\r\n",
      "  resized_input_values = sess.run(decoded_image_tensor,\r\n",
      "                                  {image_data_tensor: image_data})\r\n",
      "  \u001b[37m# Then run it through the recognition network.\u001b[39;49;00m\r\n",
      "  bottleneck_values = sess.run(bottleneck_tensor,\r\n",
      "                               {resized_input_tensor: resized_input_values})\r\n",
      "  bottleneck_values = np.squeeze(bottleneck_values)\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m bottleneck_values\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmaybe_download_and_extract\u001b[39;49;00m(data_url):\r\n",
      "  \u001b[33m\"\"\"Download and extract model tar file.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  If the pretrained model we're using doesn't already exist, this function\u001b[39;49;00m\r\n",
      "\u001b[33m  downloads it from the TensorFlow.org website and unpacks it into a directory.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    data_url: Web location of the tar file containing the pretrained model.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  dest_directory = FLAGS.model_dir\r\n",
      "  \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(dest_directory):\r\n",
      "    os.makedirs(dest_directory)\r\n",
      "  filename = data_url.split(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)[-\u001b[34m1\u001b[39;49;00m]\r\n",
      "  filepath = os.path.join(dest_directory, filename)\r\n",
      "  \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(filepath):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_progress\u001b[39;49;00m(count, block_size, total_size):\r\n",
      "      sys.stdout.write(\u001b[33m'\u001b[39;49;00m\u001b[33m\\r\u001b[39;49;00m\u001b[33m>> Downloading \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m%.1f\u001b[39;49;00m\u001b[33m%%\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\r\n",
      "                       (filename,\r\n",
      "                        \u001b[36mfloat\u001b[39;49;00m(count * block_size) / \u001b[36mfloat\u001b[39;49;00m(total_size) * \u001b[34m100.0\u001b[39;49;00m))\r\n",
      "      sys.stdout.flush()\r\n",
      "\r\n",
      "    filepath, _ = urllib.request.urlretrieve(data_url, filepath, _progress)\r\n",
      "    \u001b[34mprint\u001b[39;49;00m()\r\n",
      "    statinfo = os.stat(filepath)\r\n",
      "    tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mSuccessfully downloaded\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, filename, statinfo.st_size,\r\n",
      "                    \u001b[33m'\u001b[39;49;00m\u001b[33mbytes.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "  tarfile.open(filepath, \u001b[33m'\u001b[39;49;00m\u001b[33mr:gz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).extractall(dest_directory)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mensure_dir_exists\u001b[39;49;00m(dir_name):\r\n",
      "  \u001b[33m\"\"\"Makes sure the folder exists on disk.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    dir_name: Path string to the folder we want to create.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(dir_name):\r\n",
      "    os.makedirs(dir_name)\r\n",
      "\r\n",
      "\r\n",
      "bottleneck_path_2_bottleneck_values = {}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_bottleneck_file\u001b[39;49;00m(bottleneck_path, image_lists, label_name, index,\r\n",
      "                           image_dir, category, sess, jpeg_data_tensor,\r\n",
      "                           decoded_image_tensor, resized_input_tensor,\r\n",
      "                           bottleneck_tensor):\r\n",
      "  \u001b[33m\"\"\"Create a single bottleneck file.\"\"\"\u001b[39;49;00m\r\n",
      "  tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating bottleneck at \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + bottleneck_path)\r\n",
      "  image_path = get_image_path(image_lists, label_name, index,\r\n",
      "                              image_dir, category)\r\n",
      "  \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m gfile.Exists(image_path):\r\n",
      "    tf.logging.fatal(\u001b[33m'\u001b[39;49;00m\u001b[33mFile does not exist \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, image_path)\r\n",
      "  image_data = gfile.FastGFile(image_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).read()\r\n",
      "  \u001b[34mtry\u001b[39;49;00m:\r\n",
      "    bottleneck_values = run_bottleneck_on_image(\r\n",
      "        sess, image_data, jpeg_data_tensor, decoded_image_tensor,\r\n",
      "        resized_input_tensor, bottleneck_tensor)\r\n",
      "  \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mError during processing file \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m (\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (image_path,\r\n",
      "                                                                 \u001b[36mstr\u001b[39;49;00m(e)))\r\n",
      "  bottleneck_string = \u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.join(\u001b[36mstr\u001b[39;49;00m(x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m bottleneck_values)\r\n",
      "  \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(bottleneck_path, \u001b[33m'\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m bottleneck_file:\r\n",
      "    bottleneck_file.write(bottleneck_string)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_or_create_bottleneck\u001b[39;49;00m(sess, image_lists, label_name, index, image_dir,\r\n",
      "                             category, bottleneck_dir, jpeg_data_tensor,\r\n",
      "                             decoded_image_tensor, resized_input_tensor,\r\n",
      "                             bottleneck_tensor, architecture):\r\n",
      "  \u001b[33m\"\"\"Retrieves or calculates bottleneck values for an image.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  If a cached version of the bottleneck data exists on-disk, return that,\u001b[39;49;00m\r\n",
      "\u001b[33m  otherwise calculate the data and save it to disk for future use.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    sess: The current active TensorFlow Session.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_lists: Dictionary of training images for each label.\u001b[39;49;00m\r\n",
      "\u001b[33m    label_name: Label string we want to get an image for.\u001b[39;49;00m\r\n",
      "\u001b[33m    index: Integer offset of the image we want. This will be modulo-ed by the\u001b[39;49;00m\r\n",
      "\u001b[33m    available number of images for the label, so it can be arbitrarily large.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_dir: Root folder string  of the subfolders containing the training\u001b[39;49;00m\r\n",
      "\u001b[33m    images.\u001b[39;49;00m\r\n",
      "\u001b[33m    category: Name string of which  set to pull images from - training, testing,\u001b[39;49;00m\r\n",
      "\u001b[33m    or validation.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_dir: Folder string holding cached files of bottleneck values.\u001b[39;49;00m\r\n",
      "\u001b[33m    jpeg_data_tensor: The tensor to feed loaded jpeg data into.\u001b[39;49;00m\r\n",
      "\u001b[33m    decoded_image_tensor: The output of decoding and resizing the image.\u001b[39;49;00m\r\n",
      "\u001b[33m    resized_input_tensor: The input node of the recognition graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_tensor: The output tensor for the bottleneck values.\u001b[39;49;00m\r\n",
      "\u001b[33m    architecture: The name of the model architecture.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    Numpy array of values produced by the bottleneck layer for the image.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  label_lists = image_lists[label_name]\r\n",
      "  sub_dir = label_lists[\u001b[33m'\u001b[39;49;00m\u001b[33mdir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "  sub_dir_path = os.path.join(bottleneck_dir, sub_dir)\r\n",
      "  ensure_dir_exists(sub_dir_path)\r\n",
      "  bottleneck_path = get_bottleneck_path(image_lists, label_name, index,\r\n",
      "                                        bottleneck_dir, category, architecture)\r",
      "\r\n",
      "  \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(bottleneck_path):\r\n",
      "    create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\r\n",
      "                           image_dir, category, sess, jpeg_data_tensor,\r\n",
      "                           decoded_image_tensor, resized_input_tensor,\r\n",
      "                           bottleneck_tensor)\r\n",
      "  \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(bottleneck_path, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m bottleneck_file:\r\n",
      "    bottleneck_string = bottleneck_file.read()\r\n",
      "  did_hit_error = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "  \u001b[34mtry\u001b[39;49;00m:\r\n",
      "    bottleneck_values = [\u001b[36mfloat\u001b[39;49;00m(x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m bottleneck_string.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\r\n",
      "  \u001b[34mexcept\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m:\r\n",
      "    tf.logging.warning(\u001b[33m'\u001b[39;49;00m\u001b[33mInvalid float found, recreating bottleneck\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    did_hit_error = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "  \u001b[34mif\u001b[39;49;00m did_hit_error:\r\n",
      "    create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\r\n",
      "                           image_dir, category, sess, jpeg_data_tensor,\r\n",
      "                           decoded_image_tensor, resized_input_tensor,\r\n",
      "                           bottleneck_tensor)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(bottleneck_path, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m bottleneck_file:\r\n",
      "      bottleneck_string = bottleneck_file.read()\r\n",
      "    \u001b[37m# Allow exceptions to propagate here, since they shouldn't happen after a\u001b[39;49;00m\r\n",
      "    \u001b[37m# fresh creation\u001b[39;49;00m\r\n",
      "    bottleneck_values = [\u001b[36mfloat\u001b[39;49;00m(x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m bottleneck_string.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m bottleneck_values\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcache_bottlenecks\u001b[39;49;00m(sess, image_lists, image_dir, bottleneck_dir,\r\n",
      "                      jpeg_data_tensor, decoded_image_tensor,\r\n",
      "                      resized_input_tensor, bottleneck_tensor, architecture):\r\n",
      "  \u001b[33m\"\"\"Ensures all the training, testing, and validation bottlenecks are cached.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Because we're likely to read the same image multiple times (if there are no\u001b[39;49;00m\r\n",
      "\u001b[33m  distortions applied during training) it can speed things up a lot if we\u001b[39;49;00m\r\n",
      "\u001b[33m  calculate the bottleneck layer values once for each image during\u001b[39;49;00m\r\n",
      "\u001b[33m  preprocessing, and then just read those cached values repeatedly during\u001b[39;49;00m\r\n",
      "\u001b[33m  training. Here we go through all the images we've found, calculate those\u001b[39;49;00m\r\n",
      "\u001b[33m  values, and save them off.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    sess: The current active TensorFlow Session.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_lists: Dictionary of training images for each label.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_dir: Root folder string of the subfolders containing the training\u001b[39;49;00m\r\n",
      "\u001b[33m    images.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_dir: Folder string holding cached files of bottleneck values.\u001b[39;49;00m\r\n",
      "\u001b[33m    jpeg_data_tensor: Input tensor for jpeg data from file.\u001b[39;49;00m\r\n",
      "\u001b[33m    decoded_image_tensor: The output of decoding and resizing the image.\u001b[39;49;00m\r\n",
      "\u001b[33m    resized_input_tensor: The input node of the recognition graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_tensor: The penultimate output layer of the graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    architecture: The name of the model architecture.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    Nothing.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  how_many_bottlenecks = \u001b[34m0\u001b[39;49;00m\r\n",
      "  ensure_dir_exists(bottleneck_dir)\r\n",
      "  \u001b[34mfor\u001b[39;49;00m label_name, label_lists \u001b[35min\u001b[39;49;00m image_lists.items():\r\n",
      "    \u001b[34mfor\u001b[39;49;00m category \u001b[35min\u001b[39;49;00m [\u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtesting\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]:\r\n",
      "      category_list = label_lists[category]\r\n",
      "      \u001b[34mfor\u001b[39;49;00m index, unused_base_name \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(category_list):\r\n",
      "        get_or_create_bottleneck(\r\n",
      "            sess, image_lists, label_name, index, image_dir, category,\r\n",
      "            bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\r\n",
      "            resized_input_tensor, bottleneck_tensor, architecture)\r\n",
      "\r\n",
      "        how_many_bottlenecks += \u001b[34m1\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m how_many_bottlenecks % \u001b[34m100\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\r\n",
      "          tf.logging.info(\r\n",
      "              \u001b[36mstr\u001b[39;49;00m(how_many_bottlenecks) + \u001b[33m'\u001b[39;49;00m\u001b[33m bottleneck files created.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_random_cached_bottlenecks\u001b[39;49;00m(sess, image_lists, how_many, category,\r\n",
      "                                  bottleneck_dir, image_dir, jpeg_data_tensor,\r\n",
      "                                  decoded_image_tensor, resized_input_tensor,\r\n",
      "                                  bottleneck_tensor, architecture):\r\n",
      "  \u001b[33m\"\"\"Retrieves bottleneck values for cached images.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  If no distortions are being applied, this function can retrieve the cached\u001b[39;49;00m\r\n",
      "\u001b[33m  bottleneck values directly from disk for images. It picks a random set of\u001b[39;49;00m\r\n",
      "\u001b[33m  images from the specified category.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    sess: Current TensorFlow Session.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_lists: Dictionary of training images for each label.\u001b[39;49;00m\r\n",
      "\u001b[33m    how_many: If positive, a random sample of this size will be chosen.\u001b[39;49;00m\r\n",
      "\u001b[33m    If negative, all bottlenecks will be retrieved.\u001b[39;49;00m\r\n",
      "\u001b[33m    category: Name string of which set to pull from - training, testing, or\u001b[39;49;00m\r\n",
      "\u001b[33m    validation.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_dir: Folder string holding cached files of bottleneck values.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_dir: Root folder string of the subfolders containing the training\u001b[39;49;00m\r\n",
      "\u001b[33m    images.\u001b[39;49;00m\r\n",
      "\u001b[33m    jpeg_data_tensor: The layer to feed jpeg image data into.\u001b[39;49;00m\r\n",
      "\u001b[33m    decoded_image_tensor: The output of decoding and resizing the image.\u001b[39;49;00m\r\n",
      "\u001b[33m    resized_input_tensor: The input node of the recognition graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_tensor: The bottleneck output layer of the CNN graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    architecture: The name of the model architecture.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    List of bottleneck arrays, their corresponding ground truths, and the\u001b[39;49;00m\r\n",
      "\u001b[33m    relevant filenames.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  class_count = \u001b[36mlen\u001b[39;49;00m(image_lists.keys())\r\n",
      "  bottlenecks = []\r\n",
      "  ground_truths = []\r\n",
      "  filenames = []\r\n",
      "  \u001b[34mif\u001b[39;49;00m how_many >= \u001b[34m0\u001b[39;49;00m:\r\n",
      "    \u001b[37m# Retrieve a random sample of bottlenecks.\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m unused_i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(how_many):\r\n",
      "      label_index = random.randrange(class_count)\r\n",
      "      label_name = \u001b[36mlist\u001b[39;49;00m(image_lists.keys())[label_index]\r\n",
      "      image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + \u001b[34m1\u001b[39;49;00m)\r\n",
      "      image_name = get_image_path(image_lists, label_name, image_index,\r\n",
      "                                  image_dir, category)\r\n",
      "      bottleneck = get_or_create_bottleneck(\r\n",
      "          sess, image_lists, label_name, image_index, image_dir, category,\r\n",
      "          bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\r\n",
      "          resized_input_tensor, bottleneck_tensor, architecture)\r\n",
      "      ground_truth = np.zeros(class_count, dtype=np.float32)\r\n",
      "      ground_truth[label_index] = \u001b[34m1.0\u001b[39;49;00m\r\n",
      "      bottlenecks.append(bottleneck)\r\n",
      "      ground_truths.append(ground_truth)\r\n",
      "      filenames.append(image_name)\r\n",
      "  \u001b[34melse\u001b[39;49;00m:\r\n",
      "    \u001b[37m# Retrieve all bottlenecks.\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m label_index, label_name \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(image_lists.keys()):\r\n",
      "      \u001b[34mfor\u001b[39;49;00m image_index, image_name \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(\r\n",
      "          image_lists[label_name][category]):\r\n",
      "        image_name = get_image_path(image_lists, label_name, image_index,\r\n",
      "                                    image_dir, category)\r\n",
      "        bottleneck = get_or_create_bottleneck(\r\n",
      "            sess, image_lists, label_name, image_index, image_dir, category,\r\n",
      "            bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\r\n",
      "            resized_input_tensor, bottleneck_tensor, architecture)\r\n",
      "        ground_truth = np.zeros(class_count, dtype=np.float32)\r\n",
      "        ground_truth[label_index] = \u001b[34m1.0\u001b[39;49;00m\r\n",
      "        bottlenecks.append(bottleneck)\r\n",
      "        ground_truths.append(ground_truth)\r\n",
      "        filenames.append(image_name)\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m bottlenecks, ground_truths, filenames\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_random_distorted_bottlenecks\u001b[39;49;00m(\r\n",
      "    sess, image_lists, how_many, category, image_dir, input_jpeg_tensor,\r\n",
      "    distorted_image, resized_input_tensor, bottleneck_tensor):\r\n",
      "  \u001b[33m\"\"\"Retrieves bottleneck values for training images, after distortions.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  If we're training with distortions like crops, scales, or flips, we have to\u001b[39;49;00m\r\n",
      "\u001b[33m  recalculate the full model for every image, and so we can't use cached\u001b[39;49;00m\r\n",
      "\u001b[33m  bottleneck values. Instead we find random images for the requested category,\u001b[39;49;00m\r\n",
      "\u001b[33m  run them through the distortion graph, and then the full graph to get the\u001b[39;49;00m\r\n",
      "\u001b[33m  bottleneck results for each.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    sess: Current TensorFlow Session.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_lists: Dictionary of training images for each label.\u001b[39;49;00m\r\n",
      "\u001b[33m    how_many: The integer number of bottleneck values to return.\u001b[39;49;00m\r\n",
      "\u001b[33m    category: Name string of which set of images to fetch - training, testing,\u001b[39;49;00m\r\n",
      "\u001b[33m    or validation.\u001b[39;49;00m\r\n",
      "\u001b[33m    image_dir: Root folder string of the subfolders containing the training\u001b[39;49;00m\r\n",
      "\u001b[33m    images.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_jpeg_tensor: The input layer we feed the image data to.\u001b[39;49;00m\r\n",
      "\u001b[33m    distorted_image: The output node of the distortion graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    resized_input_tensor: The input node of the recognition graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_tensor: The bottleneck output layer of the CNN graph.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    List of bottleneck arrays and their corresponding ground truths.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  class_count = \u001b[36mlen\u001b[39;49;00m(image_lists.keys())\r\n",
      "  bottlenecks = []\r\n",
      "  ground_truths = []\r\n",
      "  \u001b[34mfor\u001b[39;49;00m unused_i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(how_many):\r\n",
      "    label_index = random.randrange(class_count)\r\n",
      "    label_name = \u001b[36mlist\u001b[39;49;00m(image_lists.keys())[label_index]\r\n",
      "    image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + \u001b[34m1\u001b[39;49;00m)\r\n",
      "    image_path = get_image_path(image_lists, label_name, image_index, image_dir,\r\n",
      "                                category)\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m gfile.Exists(image_path):\r\n",
      "      tf.logging.fatal(\u001b[33m'\u001b[39;49;00m\u001b[33mFile does not exist \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, image_path)\r\n",
      "    jpeg_data = gfile.FastGFile(image_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).read()\r\n",
      "    \u001b[37m# Note that we materialize the distorted_image_data as a numpy array before\u001b[39;49;00m\r\n",
      "    \u001b[37m# sending running inference on the image. This involves 2 memory copies and\u001b[39;49;00m\r\n",
      "    \u001b[37m# might be optimized in other implementations.\u001b[39;49;00m\r\n",
      "    distorted_image_data = sess.run(distorted_image,\r\n",
      "                                    {input_jpeg_tensor: jpeg_data})\r\n",
      "    bottleneck_values = sess.run(bottleneck_tensor,\r\n",
      "                                 {resized_input_tensor: distorted_image_data})\r\n",
      "    bottleneck_values = np.squeeze(bottleneck_values)\r\n",
      "    ground_truth = np.zeros(class_count, dtype=np.float32)\r\n",
      "    ground_truth[label_index] = \u001b[34m1.0\u001b[39;49;00m\r\n",
      "    bottlenecks.append(bottleneck_values)\r\n",
      "    ground_truths.append(ground_truth)\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m bottlenecks, ground_truths\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mshould_distort_images\u001b[39;49;00m(flip_left_right, random_crop, random_scale,\r\n",
      "                          random_brightness):\r\n",
      "  \u001b[33m\"\"\"Whether any distortions are enabled, from the input flags.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    flip_left_right: Boolean whether to randomly mirror images horizontally.\u001b[39;49;00m\r\n",
      "\u001b[33m    random_crop: Integer percentage setting the total margin used around the\u001b[39;49;00m\r\n",
      "\u001b[33m    crop box.\u001b[39;49;00m\r\n",
      "\u001b[33m    random_scale: Integer percentage of how much to vary the scale by.\u001b[39;49;00m\r\n",
      "\u001b[33m    random_brightness: Integer range to randomly multiply the pixel values by.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    Boolean value indicating whether any distortions should be applied.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m (flip_left_right \u001b[35mor\u001b[39;49;00m (random_crop != \u001b[34m0\u001b[39;49;00m) \u001b[35mor\u001b[39;49;00m (random_scale != \u001b[34m0\u001b[39;49;00m) \u001b[35mor\u001b[39;49;00m\r\n",
      "          (random_brightness != \u001b[34m0\u001b[39;49;00m))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32madd_input_distortions\u001b[39;49;00m(flip_left_right, random_crop, random_scale,\r\n",
      "                          random_brightness, input_width, input_height,\r\n",
      "                          input_depth, input_mean, input_std):\r\n",
      "  \u001b[33m\"\"\"Creates the operations to apply the specified distortions.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  During training it can help to improve the results if we run the images\u001b[39;49;00m\r\n",
      "\u001b[33m  through simple distortions like crops, scales, and flips. These reflect the\u001b[39;49;00m\r\n",
      "\u001b[33m  kind of variations we expect in the real world, and so can help train the\u001b[39;49;00m\r\n",
      "\u001b[33m  model to cope with natural data more effectively. Here we take the supplied\u001b[39;49;00m\r\n",
      "\u001b[33m  parameters and construct a network of operations to apply them to an image.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Cropping\u001b[39;49;00m\r\n",
      "\u001b[33m  ~~~~~~~~\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Cropping is done by placing a bounding box at a random position in the full\u001b[39;49;00m\r\n",
      "\u001b[33m  image. The cropping parameter controls the size of that box relative to the\u001b[39;49;00m\r\n",
      "\u001b[33m  input image. If it's zero, then the box is the same size as the input and no\u001b[39;49;00m\r\n",
      "\u001b[33m  cropping is performed. If the value is 50%, then the crop box will be half the\u001b[39;49;00m\r\n",
      "\u001b[33m  width and height of the input. In a diagram it looks like this:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  <       width         >\u001b[39;49;00m\r\n",
      "\u001b[33m  +---------------------+\u001b[39;49;00m\r\n",
      "\u001b[33m  |                     |\u001b[39;49;00m\r\n",
      "\u001b[33m  |   width - crop%     |\u001b[39;49;00m\r\n",
      "\u001b[33m  |    <      >         |\u001b[39;49;00m\r\n",
      "\u001b[33m  |    +------+         |\u001b[39;49;00m\r\n",
      "\u001b[33m  |    |      |         |\u001b[39;49;00m\r\n",
      "\u001b[33m  |    |      |         |\u001b[39;49;00m\r\n",
      "\u001b[33m  |    |      |         |\u001b[39;49;00m\r\n",
      "\u001b[33m  |    +------+         |\u001b[39;49;00m\r\n",
      "\u001b[33m  |                     |\u001b[39;49;00m\r\n",
      "\u001b[33m  |                     |\u001b[39;49;00m\r\n",
      "\u001b[33m  +---------------------+\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Scaling\u001b[39;49;00m\r\n",
      "\u001b[33m  ~~~~~~~\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Scaling is a lot like cropping, except that the bounding box is always\u001b[39;49;00m\r\n",
      "\u001b[33m  centered and its size varies randomly within the given range. For example if\u001b[39;49;00m\r\n",
      "\u001b[33m  the scale percentage is zero, then the bounding box is the same size as the\u001b[39;49;00m\r\n",
      "\u001b[33m  input and no scaling is applied. If it's 50%, then the bounding box will be in\u001b[39;49;00m\r\n",
      "\u001b[33m  a random range between half the width and height and full size.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    flip_left_right: Boolean whether to randomly mirror images horizontally.\u001b[39;49;00m\r\n",
      "\u001b[33m    random_crop: Integer percentage setting the total margin used around the\u001b[39;49;00m\r\n",
      "\u001b[33m    crop box.\u001b[39;49;00m\r\n",
      "\u001b[33m    random_scale: Integer percentage of how much to vary the scale by.\u001b[39;49;00m\r\n",
      "\u001b[33m    random_brightness: Integer range to randomly multiply the pixel values by.\u001b[39;49;00m\r\n",
      "\u001b[33m    graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_width: Horizontal size of expected input image to model.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_height: Vertical size of expected input image to model.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_depth: How many channels the expected input image should have.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_mean: Pixel value that should be zero in the image for the graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_std: How much to divide the pixel values by before recognition.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    The jpeg input layer and the distorted result tensor.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  jpeg_data = tf.placeholder(tf.string, name=\u001b[33m'\u001b[39;49;00m\u001b[33mDistortJPGInput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\r\n",
      "  decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)\r\n",
      "  decoded_image_4d = tf.expand_dims(decoded_image_as_float, \u001b[34m0\u001b[39;49;00m)\r\n",
      "  margin_scale = \u001b[34m1.0\u001b[39;49;00m + (random_crop / \u001b[34m100.0\u001b[39;49;00m)\r\n",
      "  resize_scale = \u001b[34m1.0\u001b[39;49;00m + (random_scale / \u001b[34m100.0\u001b[39;49;00m)\r\n",
      "  margin_scale_value = tf.constant(margin_scale)\r\n",
      "  resize_scale_value = tf.random_uniform(tensor_shape.scalar(),\r\n",
      "                                         minval=\u001b[34m1.0\u001b[39;49;00m,\r\n",
      "                                         maxval=resize_scale)\r\n",
      "  scale_value = tf.multiply(margin_scale_value, resize_scale_value)\r\n",
      "  precrop_width = tf.multiply(scale_value, input_width)\r\n",
      "  precrop_height = tf.multiply(scale_value, input_height)\r\n",
      "  precrop_shape = tf.stack([precrop_height, precrop_width])\r\n",
      "  precrop_shape_as_int = tf.cast(precrop_shape, dtype=tf.int32)\r\n",
      "  precropped_image = tf.image.resize_bilinear(decoded_image_4d,\r\n",
      "                                              precrop_shape_as_int)\r\n",
      "  precropped_image_3d = tf.squeeze(precropped_image, squeeze_dims=[\u001b[34m0\u001b[39;49;00m])\r\n",
      "  cropped_image = tf.random_crop(precropped_image_3d,\r\n",
      "                                 [input_height, input_width, input_depth])\r\n",
      "  \u001b[34mif\u001b[39;49;00m flip_left_right:\r\n",
      "    flipped_image = tf.image.random_flip_left_right(cropped_image)\r\n",
      "  \u001b[34melse\u001b[39;49;00m:\r\n",
      "    flipped_image = cropped_image\r\n",
      "  brightness_min = \u001b[34m1.0\u001b[39;49;00m - (random_brightness / \u001b[34m100.0\u001b[39;49;00m)\r\n",
      "  brightness_max = \u001b[34m1.0\u001b[39;49;00m + (random_brightness / \u001b[34m100.0\u001b[39;49;00m)\r\n",
      "  brightness_value = tf.random_uniform(tensor_shape.scalar(),\r\n",
      "                                       minval=brightness_min,\r\n",
      "                                       maxval=brightness_max)\r\n",
      "  brightened_image = tf.multiply(flipped_image, brightness_value)\r\n",
      "  offset_image = tf.subtract(brightened_image, input_mean)\r\n",
      "  mul_image = tf.multiply(offset_image, \u001b[34m1.0\u001b[39;49;00m / input_std)\r\n",
      "  distort_result = tf.expand_dims(mul_image, \u001b[34m0\u001b[39;49;00m, name=\u001b[33m'\u001b[39;49;00m\u001b[33mDistortResult\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m jpeg_data, distort_result\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mvariable_summaries\u001b[39;49;00m(var):\r\n",
      "  \u001b[33m\"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33msummaries\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    mean = tf.reduce_mean(var)\r\n",
      "    tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, mean)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33mstddev\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\r\n",
      "    tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mstddev\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, stddev)\r\n",
      "    tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, tf.reduce_max(var))\r\n",
      "    tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mmin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, tf.reduce_min(var))\r\n",
      "    tf.summary.histogram(\u001b[33m'\u001b[39;49;00m\u001b[33mhistogram\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, var)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32madd_final_training_ops\u001b[39;49;00m(class_count, final_tensor_name, bottleneck_tensor,\r\n",
      "                           bottleneck_tensor_size):\r\n",
      "  \u001b[33m\"\"\"Adds a new softmax and fully-connected layer for training.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  We need to retrain the top layer to identify our new classes, so this function\u001b[39;49;00m\r\n",
      "\u001b[33m  adds the right operations to the graph, along with some variables to hold the\u001b[39;49;00m\r\n",
      "\u001b[33m  weights, and then sets up all the gradients for the backward pass.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  The set up for the softmax and fully-connected layers is based on:\u001b[39;49;00m\r\n",
      "\u001b[33m  https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    class_count: Integer of how many categories of things we're trying to\u001b[39;49;00m\r\n",
      "\u001b[33m    recognize.\u001b[39;49;00m\r\n",
      "\u001b[33m    final_tensor_name: Name string for the new final node that produces results.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_tensor: The output of the main CNN graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck_tensor_size: How many entries in the bottleneck vector.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    The tensors for the training and cross entropy results, and tensors for the\u001b[39;49;00m\r\n",
      "\u001b[33m    bottleneck input and ground truth input.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33minput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    bottleneck_input = tf.placeholder_with_default(\r\n",
      "        bottleneck_tensor,\r\n",
      "        shape=[\u001b[36mNone\u001b[39;49;00m, bottleneck_tensor_size],\r\n",
      "        name=\u001b[33m'\u001b[39;49;00m\u001b[33mBottleneckInputPlaceholder\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    ground_truth_input = tf.placeholder(tf.float32,\r\n",
      "                                        [\u001b[36mNone\u001b[39;49;00m, class_count],\r\n",
      "                                        name=\u001b[33m'\u001b[39;49;00m\u001b[33mGroundTruthInput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "  \u001b[37m# Organizing the following ops as `final_training_ops` so they're easier\u001b[39;49;00m\r\n",
      "  \u001b[37m# to see in TensorBoard\u001b[39;49;00m\r\n",
      "  layer_name = \u001b[33m'\u001b[39;49;00m\u001b[33mfinal_training_ops\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  \u001b[34mwith\u001b[39;49;00m tf.name_scope(layer_name):\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33mweights\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "      initial_value = tf.truncated_normal(\r\n",
      "          [bottleneck_tensor_size, class_count], stddev=\u001b[34m0.001\u001b[39;49;00m)\r\n",
      "\r\n",
      "      layer_weights = tf.Variable(initial_value, name=\u001b[33m'\u001b[39;49;00m\u001b[33mfinal_weights\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "      variable_summaries(layer_weights)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33mbiases\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "      layer_biases = tf.Variable(tf.zeros([class_count]), name=\u001b[33m'\u001b[39;49;00m\u001b[33mfinal_biases\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "      variable_summaries(layer_biases)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33mWx_plus_b\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "      logits = tf.matmul(bottleneck_input, layer_weights) + layer_biases\r\n",
      "      tf.summary.histogram(\u001b[33m'\u001b[39;49;00m\u001b[33mpre_activations\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, logits)\r\n",
      "\r\n",
      "  final_tensor = tf.nn.softmax(logits, name=final_tensor_name)\r\n",
      "  tf.summary.histogram(\u001b[33m'\u001b[39;49;00m\u001b[33mactivations\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, final_tensor)\r\n",
      "\r\n",
      "  \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33mcross_entropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\r\n",
      "        labels=ground_truth_input, logits=logits)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "      cross_entropy_mean = tf.reduce_mean(cross_entropy)\r\n",
      "  tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mcross_entropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, cross_entropy_mean)\r\n",
      "\r\n",
      "  \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\r\n",
      "    train_step = optimizer.minimize(cross_entropy_mean)\r\n",
      "\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m (train_step, cross_entropy_mean, bottleneck_input, ground_truth_input,\r\n",
      "          final_tensor)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32madd_evaluation_step\u001b[39;49;00m(result_tensor, ground_truth_tensor):\r\n",
      "  \u001b[33m\"\"\"Inserts the operations we need to evaluate the accuracy of our results.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    result_tensor: The new final node that produces results.\u001b[39;49;00m\r\n",
      "\u001b[33m    ground_truth_tensor: The node we feed ground truth data\u001b[39;49;00m\r\n",
      "\u001b[33m    into.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    Tuple of (evaluation step, prediction).\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33mcorrect_prediction\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "      prediction = tf.argmax(result_tensor, \u001b[34m1\u001b[39;49;00m)\r\n",
      "      correct_prediction = tf.equal(\r\n",
      "          prediction, tf.argmax(ground_truth_tensor, \u001b[34m1\u001b[39;49;00m))\r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.name_scope(\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "      evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n",
      "  tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, evaluation_step)\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m evaluation_step, prediction\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_graph_to_file\u001b[39;49;00m(sess, graph, graph_file_name):\r\n",
      "  output_graph_def = graph_util.convert_variables_to_constants(\r\n",
      "      sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\r\n",
      "  \u001b[34mwith\u001b[39;49;00m gfile.FastGFile(graph_file_name, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "    f.write(output_graph_def.SerializeToString())\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprepare_file_system\u001b[39;49;00m():\r\n",
      "  \u001b[37m# Setup the directory we'll write summaries to for TensorBoard\u001b[39;49;00m\r\n",
      "  \u001b[34mif\u001b[39;49;00m tf.gfile.Exists(FLAGS.summaries_dir):\r\n",
      "    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\r\n",
      "  tf.gfile.MakeDirs(FLAGS.summaries_dir)\r\n",
      "  \u001b[34mif\u001b[39;49;00m FLAGS.intermediate_store_frequency > \u001b[34m0\u001b[39;49;00m:\r\n",
      "    ensure_dir_exists(FLAGS.intermediate_output_graphs_dir)\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_model_info\u001b[39;49;00m(architecture):\r\n",
      "  \u001b[33m\"\"\"Given the name of a model architecture, returns information about it.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  There are different base image recognition pretrained models that can be\u001b[39;49;00m\r\n",
      "\u001b[33m  retrained using transfer learning, and this function translates from the name\u001b[39;49;00m\r\n",
      "\u001b[33m  of a model to the attributes that are needed to download and train with it.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    architecture: Name of a model architecture.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    Dictionary of information about the model, or None if the name isn't\u001b[39;49;00m\r\n",
      "\u001b[33m    recognized\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Raises:\u001b[39;49;00m\r\n",
      "\u001b[33m    ValueError: If architecture name is unknown.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  architecture = architecture.lower()\r\n",
      "  \u001b[34mif\u001b[39;49;00m architecture == \u001b[33m'\u001b[39;49;00m\u001b[33minception_v3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    \u001b[37m# pylint: disable=line-too-long\u001b[39;49;00m\r\n",
      "    data_url = \u001b[33m'\u001b[39;49;00m\u001b[33mhttp://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    \u001b[37m# pylint: enable=line-too-long\u001b[39;49;00m\r\n",
      "    bottleneck_tensor_name = \u001b[33m'\u001b[39;49;00m\u001b[33mpool_3/_reshape:0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    bottleneck_tensor_size = \u001b[34m2048\u001b[39;49;00m\r\n",
      "    input_width = \u001b[34m299\u001b[39;49;00m\r\n",
      "    input_height = \u001b[34m299\u001b[39;49;00m\r\n",
      "    input_depth = \u001b[34m3\u001b[39;49;00m\r\n",
      "    resized_input_tensor_name = \u001b[33m'\u001b[39;49;00m\u001b[33mMul:0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    model_file_name = \u001b[33m'\u001b[39;49;00m\u001b[33mclassify_image_graph_def.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    input_mean = \u001b[34m128\u001b[39;49;00m\r\n",
      "    input_std = \u001b[34m128\u001b[39;49;00m\r\n",
      "  \u001b[34melif\u001b[39;49;00m architecture.startswith(\u001b[33m'\u001b[39;49;00m\u001b[33mmobilenet_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    parts = architecture.split(\u001b[33m'\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(parts) != \u001b[34m3\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(parts) != \u001b[34m4\u001b[39;49;00m:\r\n",
      "      tf.logging.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mCouldn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt understand architecture name \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "                       architecture)\r\n",
      "      \u001b[34mreturn\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\r\n",
      "    version_string = parts[\u001b[34m1\u001b[39;49;00m]\r\n",
      "    \u001b[34mif\u001b[39;49;00m (version_string != \u001b[33m'\u001b[39;49;00m\u001b[33m1.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m version_string != \u001b[33m'\u001b[39;49;00m\u001b[33m0.75\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m\r\n",
      "        version_string != \u001b[33m'\u001b[39;49;00m\u001b[33m0.50\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m version_string != \u001b[33m'\u001b[39;49;00m\u001b[33m0.25\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "      tf.logging.error(\r\n",
      "          \u001b[33m\"\"\"\"The Mobilenet version should be '1.0', '0.75', '0.50', or '0.25',\u001b[39;49;00m\r\n",
      "\u001b[33m  but found '%s' for architecture '%s'\"\"\"\u001b[39;49;00m,\r\n",
      "          version_string, architecture)\r\n",
      "      \u001b[34mreturn\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\r\n",
      "    size_string = parts[\u001b[34m2\u001b[39;49;00m]\r\n",
      "    \u001b[34mif\u001b[39;49;00m (size_string != \u001b[33m'\u001b[39;49;00m\u001b[33m224\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m size_string != \u001b[33m'\u001b[39;49;00m\u001b[33m192\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m\r\n",
      "        size_string != \u001b[33m'\u001b[39;49;00m\u001b[33m160\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m size_string != \u001b[33m'\u001b[39;49;00m\u001b[33m128\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "      tf.logging.error(\r\n",
      "          \u001b[33m\"\"\"The Mobilenet input size should be '224', '192', '160', or '128',\u001b[39;49;00m\r\n",
      "\u001b[33m but found '%s' for architecture '%s'\"\"\"\u001b[39;49;00m,\r\n",
      "          size_string, architecture)\r\n",
      "      \u001b[34mreturn\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(parts) == \u001b[34m3\u001b[39;49;00m:\r\n",
      "      is_quantized = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "      \u001b[34mif\u001b[39;49;00m parts[\u001b[34m3\u001b[39;49;00m] != \u001b[33m'\u001b[39;49;00m\u001b[33mquantized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        tf.logging.error(\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mCouldn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt understand architecture suffix \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m for \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, parts[\u001b[34m3\u001b[39;49;00m],\r\n",
      "            architecture)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\r\n",
      "      is_quantized = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "    data_url = \u001b[33m'\u001b[39;49;00m\u001b[33mhttp://download.tensorflow.org/models/mobilenet_v1_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    data_url += version_string + \u001b[33m'\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + size_string + \u001b[33m'\u001b[39;49;00m\u001b[33m_frozen.tgz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    bottleneck_tensor_name = \u001b[33m'\u001b[39;49;00m\u001b[33mMobilenetV1/Predictions/Reshape:0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    bottleneck_tensor_size = \u001b[34m1001\u001b[39;49;00m\r\n",
      "    input_width = \u001b[36mint\u001b[39;49;00m(size_string)\r\n",
      "    input_height = \u001b[36mint\u001b[39;49;00m(size_string)\r\n",
      "    input_depth = \u001b[34m3\u001b[39;49;00m\r\n",
      "    resized_input_tensor_name = \u001b[33m'\u001b[39;49;00m\u001b[33minput:0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_quantized:\r\n",
      "      model_base_name = \u001b[33m'\u001b[39;49;00m\u001b[33mquantized_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "      model_base_name = \u001b[33m'\u001b[39;49;00m\u001b[33mfrozen_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    model_dir_name = \u001b[33m'\u001b[39;49;00m\u001b[33mmobilenet_v1_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + version_string + \u001b[33m'\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + size_string\r\n",
      "    model_file_name = os.path.join(model_dir_name, model_base_name)\r\n",
      "    input_mean = \u001b[34m127.5\u001b[39;49;00m\r\n",
      "    input_std = \u001b[34m127.5\u001b[39;49;00m\r\n",
      "  \u001b[34melse\u001b[39;49;00m:\r\n",
      "    tf.logging.error(\u001b[33m\"\u001b[39;49;00m\u001b[33mCouldn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt understand architecture name \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, architecture)\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mUnknown architecture\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, architecture)\r\n",
      "\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m {\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33mdata_url\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: data_url,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33mbottleneck_tensor_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: bottleneck_tensor_name,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33mbottleneck_tensor_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: bottleneck_tensor_size,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33minput_width\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_width,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33minput_height\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_height,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33minput_depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_depth,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33mresized_input_tensor_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: resized_input_tensor_name,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_file_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: model_file_name,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33minput_mean\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_mean,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33minput_std\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_std,\r\n",
      "  }\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32madd_jpeg_decoding\u001b[39;49;00m(input_width, input_height, input_depth, input_mean,\r\n",
      "                      input_std):\r\n",
      "  \u001b[33m\"\"\"Adds operations that perform JPEG decoding and resizing to the graph..\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Args:\u001b[39;49;00m\r\n",
      "\u001b[33m    input_width: Desired width of the image fed into the recognizer graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_height: Desired width of the image fed into the recognizer graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_depth: Desired channels of the image fed into the recognizer graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_mean: Pixel value that should be zero in the image for the graph.\u001b[39;49;00m\r\n",
      "\u001b[33m    input_std: How much to divide the pixel values by before recognition.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m  Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    Tensors for the node to feed JPEG data into, and the output of the\u001b[39;49;00m\r\n",
      "\u001b[33m      preprocessing steps.\u001b[39;49;00m\r\n",
      "\u001b[33m  \"\"\"\u001b[39;49;00m\r\n",
      "  jpeg_data = tf.placeholder(tf.string, name=\u001b[33m'\u001b[39;49;00m\u001b[33mDecodeJPGInput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\r\n",
      "  decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)\r\n",
      "  decoded_image_4d = tf.expand_dims(decoded_image_as_float, \u001b[34m0\u001b[39;49;00m)\r\n",
      "  resize_shape = tf.stack([input_height, input_width])\r\n",
      "  resize_shape_as_int = tf.cast(resize_shape, dtype=tf.int32)\r\n",
      "  resized_image = tf.image.resize_bilinear(decoded_image_4d,\r\n",
      "                                           resize_shape_as_int)\r\n",
      "  offset_image = tf.subtract(resized_image, input_mean)\r\n",
      "  mul_image = tf.multiply(offset_image, \u001b[34m1.0\u001b[39;49;00m / input_std)\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m jpeg_data, mul_image\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m(_):\r\n",
      "  \u001b[37m# Needed to make sure the logging output is visible.\u001b[39;49;00m\r\n",
      "  \u001b[37m# See https://github.com/tensorflow/tensorflow/issues/3047\u001b[39;49;00m\r\n",
      "  tf.logging.set_verbosity(tf.logging.INFO)\r\n",
      "\r\n",
      "  \u001b[37m# Prepare necessary directories  that can be used during training\u001b[39;49;00m\r\n",
      "  prepare_file_system()\r\n",
      "\r\n",
      "  \u001b[37m# Gather information about the model architecture we'll be using.\u001b[39;49;00m\r\n",
      "  model_info = create_model_info(FLAGS.architecture)\r\n",
      "  \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m model_info:\r\n",
      "    tf.logging.error(\u001b[33m'\u001b[39;49;00m\u001b[33mDid not recognize architecture flag\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m -\u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[37m# Set up the pre-trained graph.\u001b[39;49;00m\r\n",
      "  maybe_download_and_extract(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mdata_url\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "  graph, bottleneck_tensor, resized_image_tensor = (\r\n",
      "      create_model_graph(model_info))\r\n",
      "\r\n",
      "  \u001b[37m# Look at the folder structure, and create lists of all the images.\u001b[39;49;00m\r\n",
      "  image_lists = create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,\r\n",
      "                                   FLAGS.validation_percentage)\r\n",
      "  class_count = \u001b[36mlen\u001b[39;49;00m(image_lists.keys())\r\n",
      "  \u001b[34mif\u001b[39;49;00m class_count == \u001b[34m0\u001b[39;49;00m:\r\n",
      "    tf.logging.error(\u001b[33m'\u001b[39;49;00m\u001b[33mNo valid folders of images found at \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + FLAGS.image_dir)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m -\u001b[34m1\u001b[39;49;00m\r\n",
      "  \u001b[34mif\u001b[39;49;00m class_count == \u001b[34m1\u001b[39;49;00m:\r\n",
      "    tf.logging.error(\u001b[33m'\u001b[39;49;00m\u001b[33mOnly one valid folder of images found at \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m +\r\n",
      "                     FLAGS.image_dir +\r\n",
      "                     \u001b[33m'\u001b[39;49;00m\u001b[33m - multiple classes are needed for classification.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m -\u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[37m# See if the command-line flags mean we're applying any distortions.\u001b[39;49;00m\r\n",
      "  do_distort_images = should_distort_images(\r\n",
      "      FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\r\n",
      "      FLAGS.random_brightness)\r\n",
      "\r\n",
      "  \u001b[34mwith\u001b[39;49;00m tf.Session(graph=graph) \u001b[34mas\u001b[39;49;00m sess:\r\n",
      "    \u001b[37m# Set up the image decoding sub-graph.\u001b[39;49;00m\r\n",
      "    jpeg_data_tensor, decoded_image_tensor = add_jpeg_decoding(\r\n",
      "        model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_width\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_height\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "        model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mean\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "        model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_std\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m do_distort_images:\r\n",
      "      \u001b[37m# We will be applying distortions, so setup the operations we'll need.\u001b[39;49;00m\r\n",
      "      (distorted_jpeg_data_tensor,\r\n",
      "       distorted_image_tensor) = add_input_distortions(\r\n",
      "           FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\r\n",
      "           FLAGS.random_brightness, model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_width\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "           model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_height\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_depth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "           model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mean\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_std\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "      \u001b[37m# We'll make sure we've calculated the 'bottleneck' image summaries and\u001b[39;49;00m\r\n",
      "      \u001b[37m# cached them on disk.\u001b[39;49;00m\r\n",
      "      cache_bottlenecks(sess, image_lists, FLAGS.image_dir,\r\n",
      "                        FLAGS.bottleneck_dir, jpeg_data_tensor,\r\n",
      "                        decoded_image_tensor, resized_image_tensor,\r\n",
      "                        bottleneck_tensor, FLAGS.architecture)\r\n",
      "\r\n",
      "    \u001b[37m# Add the new layer that we'll be training.\u001b[39;49;00m\r\n",
      "    (train_step, cross_entropy, bottleneck_input, ground_truth_input,\r\n",
      "     final_tensor) = add_final_training_ops(\r\n",
      "         \u001b[36mlen\u001b[39;49;00m(image_lists.keys()), FLAGS.final_tensor_name, bottleneck_tensor,\r\n",
      "         model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mbottleneck_tensor_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[37m# Create the operations we need to evaluate the accuracy of our new layer.\u001b[39;49;00m\r\n",
      "    evaluation_step, prediction = add_evaluation_step(\r\n",
      "        final_tensor, ground_truth_input)\r\n",
      "\r\n",
      "    \u001b[37m# Merge all the summaries and write them out to the summaries_dir\u001b[39;49;00m\r\n",
      "    merged = tf.summary.merge_all()\r\n",
      "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                         sess.graph)\r\n",
      "\r\n",
      "    validation_writer = tf.summary.FileWriter(\r\n",
      "        FLAGS.summaries_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Set up all our weights to their initial default values.\u001b[39;49;00m\r\n",
      "    init = tf.global_variables_initializer()\r\n",
      "    sess.run(init)\r\n",
      "\r\n",
      "    \u001b[37m# Run the training for as many cycles as requested on the command line.\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(FLAGS.how_many_training_steps):\r\n",
      "      \u001b[37m# Get a batch of input bottleneck values, either calculated fresh every\u001b[39;49;00m\r\n",
      "      \u001b[37m# time with distortions applied, or from the cache stored on disk.\u001b[39;49;00m\r\n",
      "      \u001b[34mif\u001b[39;49;00m do_distort_images:\r\n",
      "        (train_bottlenecks,\r\n",
      "         train_ground_truth) = get_random_distorted_bottlenecks(\r\n",
      "             sess, image_lists, FLAGS.train_batch_size, \u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "             FLAGS.image_dir, distorted_jpeg_data_tensor,\r\n",
      "             distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\r\n",
      "      \u001b[34melse\u001b[39;49;00m:\r\n",
      "        (train_bottlenecks,\r\n",
      "         train_ground_truth, _) = get_random_cached_bottlenecks(\r\n",
      "             sess, image_lists, FLAGS.train_batch_size, \u001b[33m'\u001b[39;49;00m\u001b[33mtraining\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "             FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\r\n",
      "             decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\r\n",
      "             FLAGS.architecture)\r\n",
      "      \u001b[37m# Feed the bottlenecks and ground truth into the graph, and run a training\u001b[39;49;00m\r\n",
      "      \u001b[37m# step. Capture training summaries for TensorBoard with the `merged` op.\u001b[39;49;00m\r\n",
      "      train_summary, _ = sess.run(\r\n",
      "          [merged, train_step],\r\n",
      "          feed_dict={bottleneck_input: train_bottlenecks,\r\n",
      "                     ground_truth_input: train_ground_truth})\r\n",
      "      train_writer.add_summary(train_summary, i)\r\n",
      "\r\n",
      "      \u001b[37m# Every so often, print out how well the graph is training.\u001b[39;49;00m\r\n",
      "      is_last_step = (i + \u001b[34m1\u001b[39;49;00m == FLAGS.how_many_training_steps)\r\n",
      "      \u001b[34mif\u001b[39;49;00m (i % FLAGS.eval_step_interval) == \u001b[34m0\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m is_last_step:\r\n",
      "        train_accuracy, cross_entropy_value = sess.run(\r\n",
      "            [evaluation_step, cross_entropy],\r\n",
      "            feed_dict={bottleneck_input: train_bottlenecks,\r\n",
      "                       ground_truth_input: train_ground_truth})\r\n",
      "        tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m: Step \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m: Train accuracy = \u001b[39;49;00m\u001b[33m%.1f\u001b[39;49;00m\u001b[33m%%\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\r\n",
      "                        (datetime.now(), i, train_accuracy * \u001b[34m100\u001b[39;49;00m))\r\n",
      "        tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m: Step \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m: Cross entropy = \u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\r\n",
      "                        (datetime.now(), i, cross_entropy_value))\r\n",
      "        validation_bottlenecks, validation_ground_truth, _ = (\r\n",
      "            get_random_cached_bottlenecks(\r\n",
      "                sess, image_lists, FLAGS.validation_batch_size, \u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\r\n",
      "                decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\r\n",
      "                FLAGS.architecture))\r\n",
      "        \u001b[37m# Run a validation step and capture training summaries for TensorBoard\u001b[39;49;00m\r\n",
      "        \u001b[37m# with the `merged` op.\u001b[39;49;00m\r\n",
      "        validation_summary, validation_accuracy = sess.run(\r\n",
      "            [merged, evaluation_step],\r\n",
      "            feed_dict={bottleneck_input: validation_bottlenecks,\r\n",
      "                       ground_truth_input: validation_ground_truth})\r\n",
      "        validation_writer.add_summary(validation_summary, i)\r\n",
      "        tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m: Step \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m: Validation accuracy = \u001b[39;49;00m\u001b[33m%.1f\u001b[39;49;00m\u001b[33m%%\u001b[39;49;00m\u001b[33m (N=\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\r\n",
      "                        (datetime.now(), i, validation_accuracy * \u001b[34m100\u001b[39;49;00m,\r\n",
      "                         \u001b[36mlen\u001b[39;49;00m(validation_bottlenecks)))\r\n",
      "\r\n",
      "      \u001b[37m# Store intermediate results\u001b[39;49;00m\r\n",
      "      intermediate_frequency = FLAGS.intermediate_store_frequency\r\n",
      "\r\n",
      "      \u001b[34mif\u001b[39;49;00m (intermediate_frequency > \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m (i % intermediate_frequency == \u001b[34m0\u001b[39;49;00m)\r\n",
      "          \u001b[35mand\u001b[39;49;00m i > \u001b[34m0\u001b[39;49;00m):\r\n",
      "        intermediate_file_name = (FLAGS.intermediate_output_graphs_dir +\r\n",
      "                                  \u001b[33m'\u001b[39;49;00m\u001b[33mintermediate_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + \u001b[36mstr\u001b[39;49;00m(i) + \u001b[33m'\u001b[39;49;00m\u001b[33m.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mSave intermediate result to : \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m +\r\n",
      "                        intermediate_file_name)\r\n",
      "        save_graph_to_file(sess, graph, intermediate_file_name)\r\n",
      "\r\n",
      "    \u001b[37m# We've completed all our training, so run a final test evaluation on\u001b[39;49;00m\r\n",
      "    \u001b[37m# some new images we haven't used before.\u001b[39;49;00m\r\n",
      "    test_bottlenecks, test_ground_truth, test_filenames = (\r\n",
      "        get_random_cached_bottlenecks(\r\n",
      "            sess, image_lists, FLAGS.test_batch_size, \u001b[33m'\u001b[39;49;00m\u001b[33mtesting\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\r\n",
      "            decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\r\n",
      "            FLAGS.architecture))\r\n",
      "    test_accuracy, predictions = sess.run(\r\n",
      "        [evaluation_step, prediction],\r\n",
      "        feed_dict={bottleneck_input: test_bottlenecks,\r\n",
      "                   ground_truth_input: test_ground_truth})\r\n",
      "    tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mFinal test accuracy = \u001b[39;49;00m\u001b[33m%.1f\u001b[39;49;00m\u001b[33m%%\u001b[39;49;00m\u001b[33m (N=\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\r\n",
      "                    (test_accuracy * \u001b[34m100\u001b[39;49;00m, \u001b[36mlen\u001b[39;49;00m(test_bottlenecks)))\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m FLAGS.print_misclassified_test_images:\r\n",
      "      tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33m=== MISCLASSIFIED TEST IMAGES ===\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "      \u001b[34mfor\u001b[39;49;00m i, test_filename \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(test_filenames):\r\n",
      "        \u001b[34mif\u001b[39;49;00m predictions[i] != test_ground_truth[i].argmax():\r\n",
      "          tf.logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33m%70s\u001b[39;49;00m\u001b[33m  \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\r\n",
      "                          (test_filename,\r\n",
      "                           \u001b[36mlist\u001b[39;49;00m(image_lists.keys())[predictions[i]]))\r\n",
      "\r\n",
      "    \u001b[37m# Write out the trained graph and labels with the weights stored as\u001b[39;49;00m\r\n",
      "    \u001b[37m# constants.\u001b[39;49;00m\r\n",
      "    save_graph_to_file(sess, graph, FLAGS.output_graph)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m gfile.FastGFile(FLAGS.output_labels, \u001b[33m'\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "      f.write(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.join(image_lists.keys()) + \u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "  parser = argparse.ArgumentParser()\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--image_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "      default=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mPath to folders of labeled images.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--output_graph\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "      default=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/output_graph.pb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhere to save the trained graph.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--intermediate_output_graphs_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "      default=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/intermediate_graph/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhere to save the intermediate graphs.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--intermediate_store_frequency\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m         How many steps to store intermediate graph. If \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m then will not\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m         store.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--output_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "      default=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/output_labels.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhere to save the trained graph\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33ms labels.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r",
      "\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--summaries_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "      default=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/retrain_logs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhere to save summary logs for TensorBoard.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--how_many_training_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m4000\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mHow many training steps to run before ending.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m0.01\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mHow large a learning rate to use when training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--testing_percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m10\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhat percentage of images to use as a test set.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--validation_percentage\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m10\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mWhat percentage of images to use as a validation set.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--eval_step_interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m10\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mHow often to evaluate the training results.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m100\u001b[39;49;00m,\r",
      "\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mHow many images to train on at a time.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--test_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=-\u001b[34m1\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      How many images to test on. This test set is only used once, to evaluate\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      the final accuracy of the model after training completes.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      A value of -1 causes the entire test set to be used, which leads to more\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      stable results across runs.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--validation_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m100\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      How many images to use in an evaluation batch. This validation set is\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      used much more often than the test set, and is an early indicator of how\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      accurate the model is during training.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      A value of -1 causes the entire validation set to be used, which leads to\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      more stable results across training iterations, but may be slower on large\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      training sets.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--print_misclassified_test_images\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      default=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      Whether to print out a list of all misclassified test images.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m,\r\n",
      "      action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "      default=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/imagenet\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      Path to classify_image_graph_def.pb,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      imagenet_synset_to_human_label_map.txt, and\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      imagenet_2012_challenge_label_map_proto.pbtxt.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--bottleneck_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "      default=\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp/bottleneck\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m'\u001b[39;49;00m\u001b[33mPath to cache bottleneck layer values as files.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--final_tensor_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "      default=\u001b[33m'\u001b[39;49;00m\u001b[33mfinal_result\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      The name of the output classification layer in the retrained graph.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--flip_left_right\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      default=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      Whether to randomly flip half of the training images horizontally.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m,\r\n",
      "      action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--random_crop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      A percentage determining how much of a margin to randomly crop off the\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      training images.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--random_scale\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      A percentage determining how much to randomly scale up the size of the\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      training images by.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--random_brightness\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "      default=\u001b[34m0\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      A percentage determining how much to randomly multiply the training image\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      input pixels up or down by.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "  )\r\n",
      "  parser.add_argument(\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33m--architecture\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "      default=\u001b[33m'\u001b[39;49;00m\u001b[33minception_v3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      help=\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      Which model architecture to use. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33minception_v3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m is the most accurate, but\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      also the slowest. For faster or smaller models, chose a MobileNet with the\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      form \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mmobilenet_<parameter size>_<input_size>[_quantized]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m. For example,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mmobilenet_1.0_224\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m will pick a model that is 17 MB in size and takes 224\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      pixel input images, while \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mmobilenet_0.25_128_quantized\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m will choose a much\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      less accurate, but smaller and faster network that\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms 920 KB on disk and\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      takes 128x128 images. See https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m      for more information on Mobilenet.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\r\n",
      "\u001b[33m      \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m)\r\n",
      "  FLAGS, unparsed = parser.parse_known_args()\r\n",
      "  tf.app.run(main=main, argv=[sys.argv[\u001b[34m0\u001b[39;49;00m]] + unparsed)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize scripts/retrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --how_many_training_steps=500 \\\n",
    "  --model_dir=tf_files/models/ \\\n",
    "  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\n",
    "  --output_graph=tf_files/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --architecture=\"${ARCHITECTURE}\" \\\n",
    "  --image_dir=images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this step will take a while.\n",
    "\n",
    "This script downloads the pre-trained model, adds a new final layer, and trains that layer on the plankton photos you've downloaded.\n",
    "\n",
    "ImageNet does not include any of these plankton species we're training on here. However, the kinds of information that make it possible for ImageNet to differentiate among 1,000 classes are also useful for distinguishing other objects. By using this pre-trained network, we are using that information as input to the final classification layer that distinguishes our plankton classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottleneck\n",
    "\n",
    "A bottleneck is an informal term Google often use for the layer just before the final output layer that actually does the classification. \"Bottleneck\" is not used to imply that the layer is slowing down the network. We use the term bottleneck because near the output, the representation is much more compact than in the main body of the network.\n",
    "\n",
    "Every image is reused multiple times during training. Calculating the layers behind the bottleneck for each image takes a significant amount of time. Since these lower layers of the network are not being modified their outputs can be cached and reused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: I'm NOT in a hurry!\n",
    "The first retraining command iterates only 500 times. You can very likely get improved results (i.e. higher accuracy) by training for longer. To get this improvement, remove the parameter --how_many_training_steps to use the default 4,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --model_dir=tf_files/models/\"${ARCHITECTURE}\" \\\n",
    "  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\n",
    "  --output_graph=tf_files/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --architecture=\"${ARCHITECTURE}\" \\\n",
    "  --image_dir=images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training And TensorBoard\n",
    "\n",
    "Once the script finishes generating all the bottleneck files, the actual training of the final layer of the network begins.\n",
    "\n",
    "By default, this script runs 4,000 training steps. Each step chooses 10 images at random from the training set, finds their bottlenecks from the cache, and feeds them into the final layer to get predictions. Those predictions are then compared against the actual labels, and the results of this comparison is used to update the final layer's weights through a backpropagation process.\n",
    "\n",
    "As it trains, you'll see a series of step outputs, each one showing training accuracy, validation accuracy, and the cross entropy:\n",
    "\n",
    "- The training accuracy shows the percentage of the images used in the current training batch that were labeled with the correct class.\n",
    "- Validation accuracy: The validation accuracy is the precision (percentage of correctly-labelled images) on a randomly-selected group of images from a different set.\n",
    "- Cross entropy is a loss function that gives a glimpse into how well the learning process is progressing. (Lower numbers are better.)\n",
    "\n",
    "\n",
    "The figures below show an example of the progress of the model's accuracy and cross entropy as it trains. If your model has finished generating the bottleneck files you can check your model's progress by [opening TensorBoard](http://0.0.0.0:6006/), and clicking on the figure's name to show them. Ignore any warnings that TensorBoard prints to your command line.\n",
    "\n",
    "The first figure shows accuracy (y-axis) as a function of training progress (x-axis):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](./assets/train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two lines are shown. The orange line shows the accuracy of the model on the training data. While the blue line shows the accuracy on the test set (which was not used for training). This is a much better measure of the true performance of the network. If the training accuracy continues to rise while the validation accuracy decreases then the model is said to be \"overfitting\". Overfitting is when the model begins to memorize the training set instead of understanding general patterns in the data.\n",
    "\n",
    "As the process continues, you should see the reported accuracy improve. After all the training steps are complete, the script runs a final test accuracy evaluation on a set of images that are kept separate from the training and validation pictures. This test evaluation provides the best estimate of how the trained model will perform on the classification task.\n",
    "\n",
    "You should see an accuracy value of between 85% and 99%, though the exact value will vary from run to run since there's randomness in the training process. (If you are only training on two classes, you should expect higher accuracy.) This number value indicates the percentage of the images in the test set that are given the correct label after the model is fully trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Retrained Model\n",
    "\n",
    "The retraining script writes data to the following two files:\n",
    "\n",
    "- ***tf_files/retrained_graph.pb***, which contains a version of the selected network with a final layer retrained on your categories.\n",
    "- ***tf_files/retrained_labels.txt***, which is a text file containing labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying an image\n",
    "\n",
    "The codelab repo also contains a copy of tensorflow's label_image.py example, which you can use to test your network. Take a minute to read the help for this script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "usage: label_image.py [-h] [--image IMAGE] [--graph GRAPH] [--labels LABELS]\n",
      "                      [--input_height INPUT_HEIGHT]\n",
      "                      [--input_width INPUT_WIDTH] [--input_mean INPUT_MEAN]\n",
      "                      [--input_std INPUT_STD] [--input_layer INPUT_LAYER]\n",
      "                      [--output_layer OUTPUT_LAYER]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --image IMAGE         image to be processed\n",
      "  --graph GRAPH         graph/model to be executed\n",
      "  --labels LABELS       name of file containing labels\n",
      "  --input_height INPUT_HEIGHT\n",
      "                        input height\n",
      "  --input_width INPUT_WIDTH\n",
      "                        input width\n",
      "  --input_mean INPUT_MEAN\n",
      "                        input mean\n",
      "  --input_std INPUT_STD\n",
      "                        input std\n",
      "  --input_layer INPUT_LAYER\n",
      "                        name of input layer\n",
      "  --output_layer OUTPUT_LAYER\n",
      "                        name of output layer\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.label_image -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the script on this image of a Calanoida:\n",
    "\n",
    "![calanoida](./cml-plankton-classifier/images/Calanoida/10.dx5duqtumaam4og.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.label_image \\\n",
    "    --graph=tf_files/retrained_graph.pb  \\\n",
    "    --image=images/Calanoida/10.dx5duqtumaam4og.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each execution will print a list of plankton labels, in most cases with the correct plankton on top (though each retrained model may be slightly different)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might get results like this for a Calanoida photo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![results](./assets/results_Calanoida.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates a high confidence (~95%) that the image is a Calanoida, and low confidence for any other label.\n",
    "\n",
    "You can use label_image.py to classify any image file you choose, either from your downloaded collection, or new ones. You just have to change the --image file name argument to the script.\n",
    "\n",
    "![another](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Cyanea_kils.jpg/180px-Cyanea_kils.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.label_image \\\n",
    "    --graph=tf_files/retrained_graph.pb  \\\n",
    "    --image=../assets/180px-Cyanea_kils.jpg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy our Classification Model\n",
    "\n",
    "####  Flask\n",
    "\n",
    "Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries.[3] It has no database abstraction layer, form validation, or any other components where pre-existing third-party libraries provide common functions. However, Flask supports extensions that can add application features as if they were implemented in Flask itself. Extensions exist for object-relational mappers, form validation, upload handling, various open authentication technologies and several common framework related tools. Extensions are updated far more regularly than the core Flask program.[4]\n",
    "\n",
    "Applications that use the Flask framework include Pinterest,[5] LinkedIn,[6] and the community web page for Flask itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Clone the Github repository\n",
    "!git clone https://github.com/aymen-mouelhi/cml-plankton-flask.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Display the section that does classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Copy your model files\n",
    "cp tf_files/models/mobilenet_0.50_224 ./cml-plankton-flask/models/mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp tf_files/models/retrained_graph.pb ./cml-plankton-flask/models/mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp tf_files/models/retrained_labels.txt ./cml-plankton-flask/models/mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/i060307/Desktop/Projects/cml-plankton\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd cml-plankton-flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start web server\n",
    "!python app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "In order to test our chatbot, we need to deploy it in a webserver where it can be accessible from recast.ai\n",
    "This can be done by deploying our Flask application to Cloud Foundry or to aws. But, as this require a complicated setup, we can use ngrok instead, for testing purposes only. Ngrok will allow external access to your defined http(s) port and will provide an external url that can be included in recast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download ngrok\n",
    "You can get the latest version of ngrok from [here](https://ngrok.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Deploy using ngrok\n",
    "!ngrok http 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot\n",
    "\n",
    "In this part we will start creating our chatbot using SAP Conversational AI. \n",
    "\n",
    "\n",
    "### Chatbot Settings\n",
    "\n",
    "We will need to tell our chatbot from where it will get the answers for the classified plankton. Head to the settings section and insert your HTTPS link for your web server\n",
    "\n",
    "\n",
    "![configuration](./assets/configuration.png)\n",
    "\n",
    "### Intents Creation\n",
    "\n",
    "Our bot will need to have an intent for classifying plankton images. \n",
    "Let's start by creating the intent named \"classify_intent\"\n",
    "\n",
    "![intent](./assets/intent.png)\n",
    "\n",
    "Once we have created the intent, the next step is to define what should we do once we detect an intent. This is done throught the tab \"Build\".\n",
    "- Create a new skill \"handle_classify_intent\" and provide a description\n",
    "- Define when the skill should be trigerred\n",
    "- As we will be expecting a url from the user, we would need to define a requirement parameter:\n",
    "    - #url as image\n",
    "- Now the interesting part, let's define the action. In our case, the chatbot should send an HTTP call to our deployed python application. For this workshop, we won't need authentication, so just :\n",
    "    - Select the type of the HTTP request: POST\n",
    "    - type /classify as the endpoint\n",
    "    \n",
    "![skill](./assets/skill.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "![whale food](./assets/whale_food.png)\n",
    "\n",
    "That's it ! Throught this workshop, you retrained a deep learning model to classify plankton, and then you used it to asnwer users questions.\n",
    "The same technique can be used to classify flowers and create a flower clasification chatbot :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
