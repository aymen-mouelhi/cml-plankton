{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) is an open source library for numerical computation, specializing in machine learning applications.\n",
    "This part aims at using transfer learning technique to classify plankton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you will build\n",
    "\n",
    "In this codelab, you will learn how to run TensorFlow on a single machine, and will train a simple classifier to classify images of plankton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/60.image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jelly](./assets/results_jelly.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using transfer learning, which means we are starting with a model that has been already trained on another problem. We will then retrain it on a similar problem. Deep learning from scratch can take days, but transfer learning can be done in short order.\n",
    "\n",
    "We are going to use a model trained on the [ImageNet](http://image-net.org/) Large Visual Recognition Challenge [dataset](http://www.image-net.org/challenges/LSVRC/2012/). These models can differentiate between 1,000 different classes, like Dalmatian or dishwasher. You will have a choice of model architectures, so you can determine the right tradeoff between speed, size and accuracy for your problem.\n",
    "\n",
    "We will use this same model, but retrain it to tell apart a small number of classes based on our own examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you'll Learn\n",
    "\n",
    "- How to use Python and TensorFlow to train an image classifier\n",
    "- How to classify images with your trained classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you need\n",
    "\n",
    "- A basic Python knowledge\n",
    "- A basic understanding of Linux commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plankton\n",
    "\n",
    "Plankton are critically important to our ecosystem, accounting for more than half the primary productivity on earth and nearly half the total carbon fixed in the global carbon cycle. They form the foundation of aquatic food webs including those of large, important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.\n",
    "\n",
    "![plankton](https://storage.googleapis.com/kaggle-competitions/kaggle/3978/media/Plankton-Diagram3.png)\n",
    "\n",
    "Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of an underwater imagery sensor. This towed, underwater camera system captures microscopic, high-resolution images over large study areas. The images can then be analyzed to assess species populations and distributions.\n",
    "\n",
    "Manual analysis of the imagery is infeasible – it would take a year or more to manually analyze the imagery volume captured in a single day. Automated image classification using machine learning tools is an alternative to the manual approach. Analytics will allow analysis at speeds and scales previously thought impossible. The automated system will have broad applications for assessment of ocean and ecosystem health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and installation \n",
    "\n",
    "### Install TensorFlow\n",
    "\n",
    "Before we can begin the tutorial you need to [install TensorFlow](https://www.tensorflow.org/versions/r1.7/install/) version 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.7.*\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/90/f73789d9f6ecbd629ef646e234af9d15a6fadf81a928f9ae4332ba85fd76/tensorflow-1.7.1-cp36-cp36m-macosx_10_11_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.7.*) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.7.*) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.7.*) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.7.*) (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.7.*) (0.31.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.7.*) (1.14.3)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.7.*) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.7.*) (0.2.0)\n",
      "Collecting tensorboard<1.8.0,>=1.7.0 (from tensorflow==1.7.*)\n",
      "  Using cached https://files.pythonhosted.org/packages/0b/ec/65d4e8410038ca2a78c09034094403d231228d0ddcae7d470b223456e55d/tensorboard-1.7.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.1.6 in /anaconda3/lib/python3.6/site-packages (from tensorflow==1.7.*) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /anaconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow==1.7.*) (39.1.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.10 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /anaconda3/lib/python3.6/site-packages (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*) (3.0)\n",
      "Collecting bleach==1.5.0 (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*)\n",
      "  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.8.0,>=1.7.0->tensorflow==1.7.*)\n",
      "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
      "  Found existing installation: html5lib 1.0.1\n",
      "    Uninstalling html5lib-1.0.1:\n",
      "      Successfully uninstalled html5lib-1.0.1\n",
      "  Found existing installation: bleach 2.1.3\n",
      "    Uninstalling bleach-2.1.3:\n",
      "      Successfully uninstalled bleach-2.1.3\n",
      "  Found existing installation: tensorboard 1.10.0\n",
      "    Uninstalling tensorboard-1.10.0:\n",
      "      Successfully uninstalled tensorboard-1.10.0\n",
      "  Found existing installation: tensorflow 1.10.1\n",
      "    Uninstalling tensorflow-1.10.1:\n",
      "      Successfully uninstalled tensorflow-1.10.1\n",
      "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.7.0 tensorflow-1.7.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "!pip install --upgrade \"tensorflow==1.7.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the git repository\n",
    "\n",
    "All the code used in this codelab is contained in this git repository. Clone the repository and cd into it. This is where we will be working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check our working folder\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/aymen-mouelhi/cml-plankton-classifier.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd cml-plankton-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCONTRIBUTING.md\u001b[m\u001b[m \u001b[31mLICENSE\u001b[m\u001b[m         README.md       \u001b[1m\u001b[36mscripts\u001b[m\u001b[m         \u001b[1m\u001b[36mtf_files\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the training images\n",
    "\n",
    "Before you start any training, you'll need a set of images to teach the model about the new classes you want to recognize. Download the photos (791 MB) by invoking the following two commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  791M  100  791M    0     0  3332k      0  0:04:03  0:04:03 --:--:-- 2412k  2906k      0  0:04:38  0:00:17  0:04:21 1173k 2989k      0  0:04:30  0:00:37  0:03:53 3512k   0  3726k      0  0:03:37  0:00:50  0:02:47 6438k247M    0     0  4121k      0  0:03:16  0:01:01  0:02:15 5993k     0  4294k      0  0:03:08  0:01:08  0:02:00 5604kM    0     0  4706k      0  0:02:52  0:01:39  0:01:13 5949kk      0  0:02:47  0:01:55  0:00:52 5366k4459k      0  0:03:01  0:02:19  0:00:42 1977kM    0     0  4391k      0  0:03:04  0:02:24  0:00:40 2481k  0  3860k      0  0:03:29  0:03:00  0:00:29 1874k  692M    0     0  3816k      0  0:03:32  0:03:05  0:00:27 2314k   0  0:03:49  0:03:22  0:00:27  386k8M    0     0  3515k      0  0:03:50  0:03:23  0:00:27  354k 0  3472k      0  0:03:53  0:03:26  0:00:27  454k\n"
     ]
    }
   ],
   "source": [
    "# Download the training images\n",
    "!curl https://cmlplankton.s3.amazonaws.com/images.zip \\\n",
    "    | tar xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a copy of the plankton photos. Confirm the contents of your working directory by issuing the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have something like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/folder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mAequorea spp.\u001b[m\u001b[m                \u001b[1m\u001b[36mMertensia ovum\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mAmphipoda\u001b[m\u001b[m                    \u001b[1m\u001b[36mMuggiaea atlantica\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mAppendicularia (or Larvacea)\u001b[m\u001b[m \u001b[1m\u001b[36mMysidacea\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mAurelia spp.\u001b[m\u001b[m                 \u001b[1m\u001b[36mNanomia bijuga\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mBeroe forskalii\u001b[m\u001b[m              \u001b[1m\u001b[36mNematoscelis difficilis\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mBeroida\u001b[m\u001b[m                      \u001b[1m\u001b[36mNyctiphanes simplex\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mBlepharipoda occidentalis\u001b[m\u001b[m    \u001b[1m\u001b[36mObelia spp.\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCalanoida\u001b[m\u001b[m                    \u001b[1m\u001b[36mOikopleura dioica\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCarybdea marsupialis\u001b[m\u001b[m         \u001b[1m\u001b[36mOstracods\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCarybdea rastoni\u001b[m\u001b[m             \u001b[1m\u001b[36mParaphronima gracilis\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCestida\u001b[m\u001b[m                      \u001b[1m\u001b[36mPelagia noctiluca\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCestum veneris\u001b[m\u001b[m               \u001b[1m\u001b[36mPhacellophora camtschatica\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mChelophyes appendiculata\u001b[m\u001b[m     \u001b[1m\u001b[36mPhronima sedentaria\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mChrysaora achlyos\u001b[m\u001b[m            \u001b[1m\u001b[36mPhyllorhiza punctata\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mChrysaora colorata\u001b[m\u001b[m           \u001b[1m\u001b[36mPleurobrachia bachei\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCladocerans\u001b[m\u001b[m                  \u001b[1m\u001b[36mPleuroncodes planipes\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCopepods\u001b[m\u001b[m                     \u001b[1m\u001b[36mPoecilostomatoida\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCtenophores\u001b[m\u001b[m                  \u001b[1m\u001b[36mPolyorchis haplus\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCubozoa: Carybdeida\u001b[m\u001b[m          \u001b[1m\u001b[36mPolyorchis penicillatus\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCyclopoida\u001b[m\u001b[m                   \u001b[1m\u001b[36mPrimno brevidens\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCyclosalpa affinis\u001b[m\u001b[m           \u001b[1m\u001b[36mPseudevadne tergestina\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCyclosalpa bakeri\u001b[m\u001b[m            \u001b[1m\u001b[36mPtychogena spp\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mCydippida\u001b[m\u001b[m                    \u001b[1m\u001b[36mPyrosoma atlanticum\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mDecapoda\u001b[m\u001b[m                     \u001b[1m\u001b[36mPyrosomida\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mDiplostraca\u001b[m\u001b[m                  \u001b[1m\u001b[36mSalpa aspera\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mDoliolida\u001b[m\u001b[m                    \u001b[1m\u001b[36mSalpa fusiformis\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mDoliolum denticulatum\u001b[m\u001b[m        \u001b[1m\u001b[36mSalpida\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mEmerita analoga\u001b[m\u001b[m              \u001b[1m\u001b[36mScrippsia pacifica\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mEucheilota bakeri\u001b[m\u001b[m            \u001b[1m\u001b[36mScyphozoa: Rhizostomeae\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mEuphausia eximia\u001b[m\u001b[m             \u001b[1m\u001b[36mScyphozoa: Semaeostomeae\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mEuphausia gibboides\u001b[m\u001b[m          \u001b[1m\u001b[36mSergestes similis\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mEuphausia pacifica\u001b[m\u001b[m           \u001b[1m\u001b[36mSiphonophores\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mEuphausia recurva\u001b[m\u001b[m            \u001b[1m\u001b[36mSoestia zonaria\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mEuphausiacea\u001b[m\u001b[m                 \u001b[1m\u001b[36mStylocheiron longicorne\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mEvadne spinifera\u001b[m\u001b[m             \u001b[1m\u001b[36mThalia democratica\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mHarpacticoida\u001b[m\u001b[m                \u001b[1m\u001b[36mThetys vagina\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mHeteromysis odontops\u001b[m\u001b[m         \u001b[1m\u001b[36mThysanoessa gregaria\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mHydrozoa: Anthoathecatae\u001b[m\u001b[m     \u001b[1m\u001b[36mThysanoessa spinifera\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mHydrozoa: Leptothecatae\u001b[m\u001b[m      \u001b[1m\u001b[36mVelella velella\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mLeucothea pulchra\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Note: You will need to open a new terminal window and copy the instructions below.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (Re)training the network\n",
    "\n",
    "### Configure your MobileNet\n",
    "\n",
    "In this exercise, we will retrain a [MobileNet](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html). MobileNet is a a small efficient convolutional neural network. \"Convolutional\" just means that the same calculations are performed at each location in the image.\n",
    "\n",
    "The MobileNet is configurable in two ways:\n",
    "\n",
    "- Input image resolution: 128,160,192, or 224px. Unsurprisingly, feeding in a higher resolution image takes more processing time, but results in better classification accuracy.\n",
    "- The relative size of the model as a fraction of the largest MobileNet: 1.0, 0.75, 0.50, or 0.25. We will use 224 0.5 for this codelab.\n",
    "\n",
    "With the recommended settings, it typically takes only a couple of minutes to retrain on a laptop. You will pass the settings inside Linux shell variables. Set those variables in your shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE=\"mobilenet_0.50_${IMAGE_SIZE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about MobileNet performance (optional)\n",
    "\n",
    "The graph below shows the first-choice-accuracies of these configurations (y-axis), vs the number of calculations required (x-axis), and the size of the model (circle area).\n",
    "\n",
    "16 points are shown for MobileNet. For each of the 4 model sizes (circle area in the figure) there is one point for each image resolution setting. The 128px image size models are represented by the lower-left point in each set, while the 224px models are in the upper right.\n",
    "\n",
    "Other notable architectures are also included for reference. \"GoogleNet\" in this figure is \"Inception V1\" in this table.\n",
    "\n",
    "![models](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/img/70170cbb89d318b1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start TensorBoard\n",
    "\n",
    "Before starting the training, launch tensorboard in the background. TensorBoard is a monitoring and inspection tool included with tensorflow. You will use it to monitor the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir tf_files/training_summaries &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "This command will fail with the following error if you already have a tensorboard process running:\n",
    "\n",
    "<span style=\"color:red\">ERROR:tensorflow:TensorBoard attempted to bind to port 6006, but it was already in use</span>\n",
    "\n",
    "You can kill all existing TensorBoard instances with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to kill all existing TensorBoard instances\n",
    "# !pkill -f \"tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate the retraining script\n",
    "\n",
    "The retrain script is from the TensorFlow Hub repo, but it is not installed as part of the pip package. So for simplicity I've included it in the codelab repository. You can run the script using the python command. Take a minute to skim its \"help\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.retrain -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the training\n",
    "\n",
    "As noted in the introduction, ImageNet models are networks with millions of parameters that can differentiate a large number of classes. We're only training the final layer of that network, so training will end in a reasonable amount of time.\n",
    "\n",
    "Start your retraining with one big command (note the --summaries_dir option, sending training progress reports to the directory that tensorboard is monitoring) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize scripts/retrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --how_many_training_steps=500 \\\n",
    "  --model_dir=tf_files/models/ \\\n",
    "  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\n",
    "  --output_graph=tf_files/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --architecture=\"${ARCHITECTURE}\" \\\n",
    "  --image_dir=images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this step will take a while.\n",
    "\n",
    "This script downloads the pre-trained model, adds a new final layer, and trains that layer on the plankton photos you've downloaded.\n",
    "\n",
    "ImageNet does not include any of these plankton species we're training on here. However, the kinds of information that make it possible for ImageNet to differentiate among 1,000 classes are also useful for distinguishing other objects. By using this pre-trained network, we are using that information as input to the final classification layer that distinguishes our plankton classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottleneck\n",
    "\n",
    "A bottleneck is an informal term Google often use for the layer just before the final output layer that actually does the classification. \"Bottleneck\" is not used to imply that the layer is slowing down the network. We use the term bottleneck because near the output, the representation is much more compact than in the main body of the network.\n",
    "\n",
    "Every image is reused multiple times during training. Calculating the layers behind the bottleneck for each image takes a significant amount of time. Since these lower layers of the network are not being modified their outputs can be cached and reused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: I'm NOT in a hurry!\n",
    "The first retraining command iterates only 500 times. You can very likely get improved results (i.e. higher accuracy) by training for longer. To get this improvement, remove the parameter --how_many_training_steps to use the default 4,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --model_dir=tf_files/models/\"${ARCHITECTURE}\" \\\n",
    "  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\n",
    "  --output_graph=tf_files/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --architecture=\"${ARCHITECTURE}\" \\\n",
    "  --image_dir=images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training And TensorBoard\n",
    "\n",
    "Once the script finishes generating all the bottleneck files, the actual training of the final layer of the network begins.\n",
    "\n",
    "By default, this script runs 4,000 training steps. Each step chooses 10 images at random from the training set, finds their bottlenecks from the cache, and feeds them into the final layer to get predictions. Those predictions are then compared against the actual labels, and the results of this comparison is used to update the final layer's weights through a backpropagation process.\n",
    "\n",
    "As it trains, you'll see a series of step outputs, each one showing training accuracy, validation accuracy, and the cross entropy:\n",
    "\n",
    "- The training accuracy shows the percentage of the images used in the current training batch that were labeled with the correct class.\n",
    "- Validation accuracy: The validation accuracy is the precision (percentage of correctly-labelled images) on a randomly-selected group of images from a different set.\n",
    "- Cross entropy is a loss function that gives a glimpse into how well the learning process is progressing. (Lower numbers are better.)\n",
    "\n",
    "\n",
    "The figures below show an example of the progress of the model's accuracy and cross entropy as it trains. If your model has finished generating the bottleneck files you can check your model's progress by [opening TensorBoard](http://0.0.0.0:6006/), and clicking on the figure's name to show them. Ignore any warnings that TensorBoard prints to your command line.\n",
    "\n",
    "The first figure shows accuracy (y-axis) as a function of training progress (x-axis):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](./assets/train.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two lines are shown. The orange line shows the accuracy of the model on the training data. While the blue line shows the accuracy on the test set (which was not used for training). This is a much better measure of the true performance of the network. If the training accuracy continues to rise while the validation accuracy decreases then the model is said to be \"overfitting\". Overfitting is when the model begins to memorize the training set instead of understanding general patterns in the data.\n",
    "\n",
    "As the process continues, you should see the reported accuracy improve. After all the training steps are complete, the script runs a final test accuracy evaluation on a set of images that are kept separate from the training and validation pictures. This test evaluation provides the best estimate of how the trained model will perform on the classification task.\n",
    "\n",
    "You should see an accuracy value of between 85% and 99%, though the exact value will vary from run to run since there's randomness in the training process. (If you are only training on two classes, you should expect higher accuracy.) This number value indicates the percentage of the images in the test set that are given the correct label after the model is fully trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Retrained Model\n",
    "\n",
    "The retraining script writes data to the following two files:\n",
    "\n",
    "- ***tf_files/retrained_graph.pb***, which contains a version of the selected network with a final layer retrained on your categories.\n",
    "- ***tf_files/retrained_labels.txt***, which is a text file containing labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying an image\n",
    "\n",
    "The codelab repo also contains a copy of tensorflow's label_image.py example, which you can use to test your network. Take a minute to read the help for this script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.label_image -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize scripts/label_image,py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the script on this image of a Calanoida:\n",
    "\n",
    "![calanoida](./cml-plankton-classifier/images/Calanoida/10.dx5duqtumaam4og.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.label_image \\\n",
    "    --graph=tf_files/retrained_graph.pb  \\\n",
    "    --image=images/Calanoida/10.dx5duqtumaam4og.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each execution will print a list of plankton labels, in most cases with the correct plankton on top (though each retrained model may be slightly different)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might get results like this for a Calanoida photo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![results](./assets/results_Calanoida.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates a high confidence (~95%) that the image is a Calanoida, and low confidence for any other label.\n",
    "\n",
    "You can use label_image.py to classify any image file you choose, either from your downloaded collection, or new ones. You just have to change the --image file name argument to the script.\n",
    "\n",
    "![another](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Cyanea_kils.jpg/180px-Cyanea_kils.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.label_image \\\n",
    "    --graph=tf_files/retrained_graph.pb  \\\n",
    "    --image=../assets/180px-Cyanea_kils.jpg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy our Classification Model\n",
    "\n",
    "####  Flask\n",
    "\n",
    "Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries.[3] It has no database abstraction layer, form validation, or any other components where pre-existing third-party libraries provide common functions. However, Flask supports extensions that can add application features as if they were implemented in Flask itself. Extensions exist for object-relational mappers, form validation, upload handling, various open authentication technologies and several common framework related tools. Extensions are updated far more regularly than the core Flask program.[4]\n",
    "\n",
    "Applications that use the Flask framework include Pinterest,[5] LinkedIn,[6] and the community web page for Flask itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/i060307/Desktop/Projects/cml-plankton\n"
     ]
    }
   ],
   "source": [
    "# Back to parent folder\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cml-plankton-flask'...\n",
      "remote: Enumerating objects: 21, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
      "remote: Total 21 (delta 3), reused 21 (delta 3), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (21/21), done.\n"
     ]
    }
   ],
   "source": [
    "#### Clone the Github repository\n",
    "!git clone https://github.com/aymen-mouelhi/cml-plankton-flask.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./cml-plankton-flask/models/mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./cml-plankton-flask/models/mobilenet/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Copy your model files\n",
    "!cp -a cml-plankton-classifier/tf_files/models/. ./cml-plankton-flask/models/mobilenet/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp cml-plankton-classifier/tf_files/retrained_graph.pb ./cml-plankton-flask/models/mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp cml-plankton-classifier/tf_files/retrained_labels.txt ./cml-plankton-flask/models/mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/i060307/Desktop/Projects/cml-plankton/cml-plankton-flask\n"
     ]
    }
   ],
   "source": [
    "%cd cml-plankton-flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mflask\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Flask, request, Response, jsonify, send_from_directory\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mconfig\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtraceback\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\r\n",
      "port = \u001b[34m5000\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mPORT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\r\n",
      "    port = \u001b[36mint\u001b[39;49;00m(os.getenv(\u001b[33m\"\u001b[39;49;00m\u001b[33mPORT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\r\n",
      "\r\n",
      "\r\n",
      "app = Flask(\u001b[31m__name__\u001b[39;49;00m, static_url_path=\u001b[33m'\u001b[39;49;00m\u001b[33m/static\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[30;01m@app.route\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mGET\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mindex\u001b[39;49;00m():\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msocket\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mit\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms working! \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + socket.getfqdn()\r\n",
      "\r\n",
      "\u001b[30;01m@app.route\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mPOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mindexPost\u001b[39;49;00m():\r\n",
      "  \u001b[34mprint\u001b[39;49;00m(json.loads(request.get_data()))\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m jsonify(\r\n",
      "    status=\u001b[34m200\u001b[39;49;00m,\r\n",
      "    replies=[{\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33mtype\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33mcontent\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mRoger that\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    }],\r\n",
      "    conversation={\r\n",
      "      \u001b[33m'\u001b[39;49;00m\u001b[33mmemory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: { \u001b[33m'\u001b[39;49;00m\u001b[33mkey\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m }\r\n",
      "    }\r\n",
      "  )\r\n",
      "\r\n",
      "\r\n",
      "\u001b[30;01m@app.route\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/classify\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mPOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtf_classify\u001b[39;49;00m():\r\n",
      "    \u001b[37m# TODO: python -m scripts.label_image     --graph=tf_files/retrained_graph.pb      --image=test/aurelia.jpeg\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msocket\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mIn tf_classify handler from {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(socket.getfqdn()))\r\n",
      "\r\n",
      "    file_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mmodels/mobilenet/example/3475870145_685a19116d.jpg\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    file_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mhttps://www.eopugetsound.org/sites/default/files/styles/magazinewidth_592px/public/topical_article/images/moon_jellyfish.jpg?itok=Esreg6zX\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Get payload\u001b[39;49;00m\r\n",
      "    payload = request.get_json(silent=\u001b[36mTrue\u001b[39;49;00m,force=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m payload == \u001b[36mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mif\u001b[39;49;00m request.get_data() !=\u001b[36mNone\u001b[39;49;00m:\r\n",
      "            payload = json.loads(request.get_data())\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m payload != \u001b[36mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mif\u001b[39;49;00m payload.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mnlp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).get(\u001b[33m\"\u001b[39;49;00m\u001b[33mentities\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).get(\u001b[33m\"\u001b[39;49;00m\u001b[33murl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\r\n",
      "            file_name = payload.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mnlp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).get(\u001b[33m\"\u001b[39;49;00m\u001b[33mentities\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).get(\u001b[33m\"\u001b[39;49;00m\u001b[33murl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m].get(\u001b[33m\"\u001b[39;49;00m\u001b[33mraw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "            \u001b[37m# Load model file\u001b[39;49;00m\r\n",
      "            model_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mmodels/mobilenet/retrained_graph.pb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            label_file = \u001b[33m\"\u001b[39;49;00m\u001b[33mmodels/mobilenet/retrained_labels.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            input_height = \u001b[34m224\u001b[39;49;00m\r\n",
      "            input_width = \u001b[34m224\u001b[39;49;00m\r\n",
      "            input_mean = \u001b[34m128\u001b[39;49;00m\r\n",
      "            input_std = \u001b[34m128\u001b[39;49;00m\r\n",
      "            input_layer = \u001b[33m\"\u001b[39;49;00m\u001b[33minput\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "            output_layer = \u001b[33m\"\u001b[39;49;00m\u001b[33mfinal_result\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "            graph = util.load_graph(model_file)\r\n",
      "            t = util.read_tensor_from_image_file(file_name,\r\n",
      "                                            input_height=input_height,\r\n",
      "                                            input_width=input_width,\r\n",
      "                                            input_mean=input_mean,\r\n",
      "                                            input_std=input_std)\r\n",
      "\r\n",
      "            input_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mimport/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + input_layer\r\n",
      "            output_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mimport/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + output_layer\r\n",
      "            input_operation = graph.get_operation_by_name(input_name);\r\n",
      "            output_operation = graph.get_operation_by_name(output_name);\r\n",
      "\r\n",
      "            \u001b[34mwith\u001b[39;49;00m tf.Session(graph=graph) \u001b[34mas\u001b[39;49;00m sess:\r\n",
      "              start = time.time()\r\n",
      "              results = sess.run(output_operation.outputs[\u001b[34m0\u001b[39;49;00m],\r\n",
      "                                {input_operation.outputs[\u001b[34m0\u001b[39;49;00m]: t})\r\n",
      "              end=time.time()\r\n",
      "\r\n",
      "            results = np.squeeze(results)\r\n",
      "\r\n",
      "            top_k = results.argsort()[-\u001b[34m5\u001b[39;49;00m:][::-\u001b[34m1\u001b[39;49;00m]\r\n",
      "            labels = util.load_labels(label_file)\r\n",
      "\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mEvaluation time (1-image): {:.3f}s\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(end-start))\r\n",
      "            template = \u001b[33m\"\u001b[39;49;00m\u001b[33m{} (score={:0.5f})\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(top_k)\r\n",
      "\r\n",
      "            \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m top_k:\r\n",
      "              \u001b[34mprint\u001b[39;49;00m(template.format(labels[i], results[i]))\r\n",
      "\r\n",
      "            \u001b[37m# I really don't know, my best guess is []\u001b[39;49;00m\r\n",
      "\r\n",
      "            \u001b[34mif\u001b[39;49;00m results[\u001b[34m0\u001b[39;49;00m] < \u001b[34m0.1\u001b[39;49;00m:\r\n",
      "                response = \u001b[33m\"\u001b[39;49;00m\u001b[33mI really don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt know, my best guess is that this looks like a \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + labels[top_k[\u001b[34m0\u001b[39;49;00m]]\r\n",
      "            \u001b[34melse\u001b[39;49;00m:\r\n",
      "                response = \u001b[33m'\u001b[39;49;00m\u001b[33mI think this is a \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + labels[top_k[\u001b[34m0\u001b[39;49;00m]]\r\n",
      "\r\n",
      "            response =  \u001b[33m'\u001b[39;49;00m\u001b[33mI think this is a \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + labels[top_k[\u001b[34m0\u001b[39;49;00m]]\r\n",
      "\r\n",
      "            \u001b[34mreturn\u001b[39;49;00m jsonify(\r\n",
      "              status=\u001b[34m200\u001b[39;49;00m,\r\n",
      "              replies=[{\r\n",
      "                \u001b[33m'\u001b[39;49;00m\u001b[33mtype\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                \u001b[33m'\u001b[39;49;00m\u001b[33mcontent\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: response\r\n",
      "              }],\r\n",
      "              conversation={\r\n",
      "                \u001b[33m'\u001b[39;49;00m\u001b[33mmemory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: { \u001b[33m'\u001b[39;49;00m\u001b[33mplankton\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: labels[top_k[\u001b[34m0\u001b[39;49;00m]] }\r\n",
      "              }\r\n",
      "            )\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[30;01m@app.route\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/errors\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mPOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32merrors\u001b[39;49;00m():\r\n",
      "  \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33min /errors !\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "  \u001b[34mprint\u001b[39;49;00m(json.loads(request.get_data()))\r\n",
      "  \u001b[34mreturn\u001b[39;49;00m jsonify(status=\u001b[34m200\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m#app.run(port=port)\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "  \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLaunching web application...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "  \u001b[37m#app.run(host='0.0.0.0', port=port, ssl_context=('./keys/keys.key', './keys/keys.crt'))\u001b[39;49;00m\r\n",
      "  \u001b[37m#app.run(host='0.0.0.0', port=port)\u001b[39;49;00m\r\n",
      "  app.run(debug=\u001b[36mTrue\u001b[39;49;00m,host=\u001b[33m'\u001b[39;49;00m\u001b[33m0.0.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, port=port)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start web server\n",
    "!python app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "In order to test our chatbot, we need to deploy it in a webserver where it can be accessible from recast.ai\n",
    "This can be done by deploying our Flask application to Cloud Foundry or to aws. But, as this require a complicated setup, we can use ngrok instead, for testing purposes only. Ngrok will allow external access to your defined http(s) port and will provide an external url that can be included in recast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download ngrok\n",
    "You can get the latest version of ngrok from [here](https://ngrok.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Deploy using ngrok\n",
    "!ngrok http 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot\n",
    "\n",
    "In this part we will start creating our chatbot using SAP Conversational AI. \n",
    "\n",
    "\n",
    "### Chatbot Settings\n",
    "\n",
    "We will need to tell our chatbot from where it will get the answers for the classified plankton. Head to the settings section and insert your HTTPS link for your web server\n",
    "\n",
    "\n",
    "![configuration](./assets/configuration.png)\n",
    "\n",
    "### Intents Creation\n",
    "\n",
    "Our bot will need to have an intent for classifying plankton images. \n",
    "Let's start by creating the intent named \"classify_intent\"\n",
    "\n",
    "![intent](./assets/intent.png)\n",
    "\n",
    "Once we have created the intent, the next step is to define what should we do once we detect an intent. This is done throught the tab \"Build\".\n",
    "- Create a new skill \"handle_classify_intent\" and provide a description\n",
    "- Define when the skill should be trigerred\n",
    "- As we will be expecting a url from the user, we would need to define a requirement parameter:\n",
    "    - #url as image\n",
    "- Now the interesting part, let's define the action. In our case, the chatbot should send an HTTP call to our deployed python application. For this workshop, we won't need authentication, so just :\n",
    "    - Select the type of the HTTP request: POST\n",
    "    - type /classify as the endpoint\n",
    "    \n",
    "![skill](./assets/skill.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "![whale food](./assets/whale_food.png)\n",
    "\n",
    "That's it ! Throught this workshop, you retrained a deep learning model to classify plankton, and then you used it to asnwer users questions.\n",
    "The same technique can be used to classify flowers and create a flower clasification chatbot :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
